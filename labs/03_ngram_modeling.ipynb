{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Statistical Language Modeling with NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "\n",
    "- Dan Jurafsky and James H. Martin's **Speech and Language Processing** ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Covered Material\n",
    "\n",
    "- SLP\n",
    "  - [Chapter 3: N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<!-- ### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Ngrams and Ngram Counting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[n-gram](https://en.wikipedia.org/wiki/N-gram) is a contiguous sequence of $N$ items from a given sequence of text or speech. An N-gram model can model sequences, notably natural languages, by using the statistical properties of N-grams (based on N-gram counts).\n",
    "\n",
    "**Example**:\n",
    "\n",
    "- character N-grams: mice\n",
    "- word N-grams: the answer is 42\n",
    "\n",
    "|                     | 1-gram                           | 2-gram                             | 3-gram                   |\n",
    "| ------------------- | -------------------------------- | ---------------------------------- | ------------------------ |\n",
    "|                     | unigram                          | bigram                             | trigram                  |\n",
    "| _Markov Order_      | 0                                | 1                                  | 2                        |\n",
    "| _Character N-grams_ | `['m', 'i', 'c', 'e']`           | `['mi', 'ic', 'ce']`               | `['mic', 'ice']`         |\n",
    "| _Word N-grams_      | `['the', 'answer', 'is' , '42']` | `['the answer', 'answer is', ...]` | `['the answer is', ...]` |\n",
    "\n",
    "You can imagine this as a sliding window with a width of $N$ and a stride to the right of 1 over tokens or characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Counting Ngrams\n",
    "\n",
    "_Frequency List_ of a corpus is essentially a unigram count, i.e. the length of the sequences to count is 1. Indeed, N-gram count is just a generalization of _Frequency List_ in which the length of the sequences to count is given by $N$ instead of only 1 as default.\n",
    "\n",
    "Thus, with N-gram count we can compute the count by taking sequences of 2 items (bigrams), 3 items (trigrams), etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.1. Preparing Data\n",
    "\n",
    "The required data format for n-gram counting is a _list-of-lists_ (lists of sentences consisting of lists of words):\n",
    "\n",
    "```\n",
    "[\n",
    "     ['the', 'answer', 'is', '42'],\n",
    "     ['the', 'mice', 'said']\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.2. Sentence beginning & end tags\n",
    "\n",
    "> Including sentence boundary markers leads to a better model. To do that we need to augment each sentence with a special symbols for beginning and end of sentence tags (`<s>` and `</s>`, respectively). The beginning of the sentence (BOS) tag gives the bigram context of the first word; and encodes probability of a word to start a sentence. Adding the end of the sentence (EOS) tag delimits the end of a sentence, i.e. it represents the final state of the sequence. The EOS is also used to signaling when the generation of a new sequence can be halted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Moreover, for n-grams larger than 2, we’ll need to assume extra context for the contexts to the left and right of the sentence boundaries. For example, to compute trigram probabilities at the very beginning of the sentence, we can use two pseudo-words for the first trigram (i.e. `['<s>', '<s>', w1]`). Alternatively, we can use [back-off](https://en.wikipedia.org/wiki/Katz%27s_back-off_model), and use the `['<s>', w1]` bigram probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**:\n",
    "`['<s>', 'the', 'answer', 'is', '42', '</s>']`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.3. NLTK Utility Functions\n",
    "\n",
    "NLTK provides utility functions for padding sequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sent = [\"the\", \"answer\", \"is\", \"42\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'the', 'answer', 'is', '42', '</s>']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import pad_sequence\n",
    "\n",
    "list(\n",
    "    pad_sequence(\n",
    "        sent,  # input sequence\n",
    "        pad_left=True,\n",
    "        left_pad_symbol=\"<s>\",\n",
    "        pad_right=True,\n",
    "        right_pad_symbol=\"</s>\",\n",
    "        n=2,  # padding for bigrams\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another NLTK function wraps this utility function with default arguments to provide a more convenient interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'answer', 'is', '42']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "list(pad_both_ends(sent, n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.4. Extracting N-grams\n",
    "\n",
    "NLTK provides a function to extract N-grams from a sequence, which also performs padding, if required arguments are provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 'answer'), ('answer', 'is'), ('is', '42')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(sent, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', 'the'),\n",
       " ('the', 'answer'),\n",
       " ('answer', 'is'),\n",
       " ('is', '42'),\n",
       " ('42', '</s>')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = ngrams(\n",
    "    sent,\n",
    "    2,\n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\",\n",
    ")\n",
    "list(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Additionally, NLTK provides wrapper functions to extract bigrams and trigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'answer'), ('answer', 'is'), ('is', '42')]\n",
      "[('the', 'answer', 'is'), ('answer', 'is', '42')]\n",
      "[('<s>', 'the'), ('the', 'answer'), ('answer', 'is'), ('is', '42'), ('42', '</s>')]\n",
      "[('<s>', 'the'), ('the', 'answer'), ('answer', 'is'), ('is', '42'), ('42', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams\n",
    "\n",
    "print(list(bigrams(sent)))\n",
    "print(list(trigrams(sent)))\n",
    "\n",
    "print(list(bigrams(pad_both_ends(sent, n=2))))\n",
    "print(\n",
    "    list(\n",
    "        bigrams(\n",
    "            sent,\n",
    "            pad_left=True,\n",
    "            left_pad_symbol=\"<s>\",\n",
    "            pad_right=True,\n",
    "            right_pad_symbol=\"</s>\",\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.5. Extracting \"Everygrams\"\n",
    "\n",
    "To make an N-gram model more robust, it is usually also trained on lower-order N-grams (i.e. unigrams in case of bigrams).\n",
    "NLTK also provides an utility function to extract these together.\n",
    "\n",
    "Note the `max_len` argument that defines the maximum length of an N-gram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the',), ('the', 'answer'), ('the', 'answer', 'is'), ('answer',), ('answer', 'is'), ('answer', 'is', '42'), ('is',), ('is', '42'), ('42',)]\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'the'), ('<s>',), ('<s>', 'the'), ('<s>', 'the', 'answer'), ('the',), ('the', 'answer'), ('the', 'answer', 'is'), ('answer',), ('answer', 'is'), ('answer', 'is', '42'), ('is',), ('is', '42'), ('is', '42', '</s>'), ('42',), ('42', '</s>'), ('42', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import everygrams\n",
    "\n",
    "print(list(everygrams(sent, max_len=3)))\n",
    "print(list(everygrams(pad_both_ends(sent, n=3), max_len=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>',),\n",
       " ('<s>', 'the'),\n",
       " ('the',),\n",
       " ('the', 'answer'),\n",
       " ('answer',),\n",
       " ('answer', 'is'),\n",
       " ('is',),\n",
       " ('is', '42'),\n",
       " ('42',),\n",
       " ('42', '</s>'),\n",
       " ('</s>',)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ngrams = everygrams(\n",
    "    sent,\n",
    "    min_len=1,\n",
    "    max_len=2,\n",
    "    pad_left=True,\n",
    "    left_pad_symbol=\"<s>\",\n",
    "    pad_right=True,\n",
    "    right_pad_symbol=\"</s>\",\n",
    ")\n",
    "list(all_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.6. \"Flattening\" the Data\n",
    "\n",
    "For language model training and evaluation (as well as vocabulary extraction) NLTK expects the data to be a flat list.\n",
    "In python, this is done either by using `itertools.chain.from_iterable` or python list comprehension as\n",
    "\n",
    "`[element for sublist in superlist for element in sublist]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which', 'is', 'the', 'sense', 'of', 'life', '?', 'the', 'answer', 'is', '42']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "data = [\n",
    "    [\"which\", \"is\", \"the\", \"sense\", \"of\", \"life\", \"?\"],\n",
    "    [\"the\", \"answer\", \"is\", \"42\"],\n",
    "]\n",
    "list(chain.from_iterable(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which', 'is', 'the', 'sense', 'of', 'life', '?', 'the', 'answer', 'is', '42']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List Comprehension\n",
    "[token for sent in data for token in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which', 'is', 'the', 'sense', 'of', 'life', '?', 'the', 'answer', 'is', '42']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or another trick\n",
    "sum(data, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which', 'is', 'the', 'sense', 'of', 'life', '?', 'the', 'answer', 'is', '42']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "list(flatten(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.1.7. Combining the Tasks\n",
    "\n",
    "NLTK wraps both tasks:\n",
    "\n",
    "- padded `ngram` (everygram) extraction from each sentence\n",
    "- flat list creation\n",
    "\n",
    "into a convenient utility function `padded_everygram_pipeline` that returns lazy iterators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>',), ('<s>', 'which'), ('which',), ('which', 'is'), ('is',), ('is', 'the'), ('the',), ('the', 'sense'), ('sense',), ('sense', 'of'), ('of',), ('of', 'life'), ('life',), ('life', '?'), ('?',), ('?', '</s>'), ('</s>',)]\n",
      "[('<s>',), ('<s>', 'the'), ('the',), ('the', 'answer'), ('answer',), ('answer', 'is'), ('is',), ('is', '42'), ('42',), ('42', '</s>'), ('</s>',)]\n"
     ]
    }
   ],
   "source": [
    "for sent_ngrams in padded_ngrams:\n",
    "    print(list(sent_ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'which',\n",
       " 'is',\n",
       " 'the',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'life',\n",
       " '?',\n",
       " '</s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'answer',\n",
       " 'is',\n",
       " '42',\n",
       " '</s>']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(flat_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** lazy iterators (aka generators) are for one-use only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object padded_everygram_pipeline.<locals>.<genexpr> at 0x78279a1fe7a0>\n",
      "[[('<s>',), ('<s>', 'winter'), ('winter',), ('winter', 'is'), ('is',), ('is', 'coming'), ('coming',), ('coming', '</s>'), ('</s>',)]]\n",
      "[]\n",
      "[[('<s>',), ('<s>', 'winter'), ('winter',), ('winter', 'is'), ('is',), ('is', 'coming'), ('coming',), ('coming', '</s>'), ('</s>',)]]\n"
     ]
    }
   ],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, [\"winter is coming\".split()])\n",
    "print(padded_ngrams)\n",
    "print([list(x) for x in padded_ngrams])  # We use the generator\n",
    "print([list(x) for x in padded_ngrams])  # The generator is gone\n",
    "# If you want to reuse the generator you have to recompute it\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, [\"winter is coming\".split()])\n",
    "print([list(x) for x in padded_ngrams])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Ngram Counter\n",
    "\n",
    "NLTK provides NgramCounter class to do the ngram counting. The class can be initialized without any argument and then updated with `ngrams` (via `update()` method); or directly initialized with `ngrams`.\n",
    "\n",
    "`N()` method returns total number of N-grams stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import NgramCounter\n",
    "\n",
    "counter = NgramCounter()\n",
    "counter.N()  # return total number of stored ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)\n",
    "counter.update(padded_ngrams)  # update counter with ngrams\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)\n",
    "counter = NgramCounter(padded_ngrams)\n",
    "counter.N()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 1.2.1. Accessing Ngram Counts\n",
    "\n",
    "Ngram counts can be accessed using standard python dictionary notation.\n",
    "\n",
    "- Ngram order counts can be accessed using integer keys (order)\n",
    "  - returns `Frequency Distribution` or `Conditional Frequency Distribution` objects. [link](https://www.nltk.org/api/nltk.probability.html)\n",
    "- Unigram counts can be accessed using string keys.\n",
    "- Bigram counts can be accessed using list keys & string keys (see example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 11 samples and 15 outcomes>\n",
      "<ConditionalFreqDist with 10 conditions>\n"
     ]
    }
   ],
   "source": [
    "# ngram order counts\n",
    "print(counter[1])  # Frequency Distribution\n",
    "print(counter[2])  # Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 11 samples and 15 outcomes>\n"
     ]
    }
   ],
   "source": [
    "# print counts\n",
    "print(counter.unigrams)  # unigram count for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unigram counts\n",
    "counter[\"the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'full' bigram counts (full bigrams)\n",
    "counter[[\"the\"]][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'sense': 1, 'answer': 1})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get a frequency distribution over all continuations, you can use list or tuple.\n",
    "# counter[['the']] or counter[('the',)]\n",
    "counter[[\"the\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'sense': 1, 'answer': 1})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For Conditional Frequency Distributions, only tuples are accepted.\n",
    "# counter[2][['the']] with throw an error\n",
    "counter[2][(\"the\",)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "- Load Shakespeare's Hamlet from Gutenberg corpus\n",
    "\n",
    "  - lowercase it\n",
    "\n",
    "- Extract padded unigrams and bigrams\n",
    "\n",
    "- Using NgramCounter\n",
    "  - get total number of ngrams\n",
    "  - get count of unigram `the`\n",
    "  - get count of bigram `of the`\n",
    "\n",
    "|                 | Count |\n",
    "| --------------- | ----- |\n",
    "| Ngrams          | 84038 |\n",
    "| Unigram _the_   | 993   |\n",
    "| Bigram _of the_ | 59    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ngrams:  84038\n",
      "Count of unigram 'the':  993\n",
      "Count of bigram 'of the':  59\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "import nltk\n",
    "from nltk.lm import NgramCounter\n",
    "\n",
    "\n",
    "gutenberg_plain = [[w.lower() for w in sent] for sent in gutenberg.sents(\"shakespeare-hamlet.txt\") ]\n",
    "ngrams, text = padded_everygram_pipeline(2, gutenberg_plain)\n",
    "ngram_counter = NgramCounter(ngrams)\n",
    "\n",
    "print(\"Total number of ngrams: \", ngram_counter.N())\n",
    "print(\"Count of unigram 'the': \", ngram_counter[\"the\"])\n",
    "print(\"Count of bigram 'of the': \", ngram_counter[[\"of\"]][\"the\"])\n",
    "\n",
    "# double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# padded_ngrams, flat_text = padded_everygram_pipeline(2, hamlet_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "counter = NgramCounter(padded_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(counter.N())  # Total number of Ngrams\n",
    "print(counter[\"the\"])  # the\n",
    "print(counter[[\"of\"]][\"the\"])  # of the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Vocabulary and Basic Usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[`Vocabulary`](https://www.nltk.org/api/nltk.lm.vocabulary.html) class of NLTK satisfies two common language modeling requirements for a vocabulary:\n",
    "\n",
    "- When checking membership and calculating its size, filters items by comparing their counts to a cutoff value.\n",
    "- Adds a special \"unknown\" token which unseen words are mapped to.\n",
    "\n",
    "Tokens with counts greater than or equal to the cut-off value will be considered part of the vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Vocabulary(counts=None, unk_cutoff=1, unk_label='<UNK>)` create a new `Vocabulary`.\n",
    "\n",
    "- `counts` (optional) iterable or collections.\n",
    "\n",
    "  - Counter instance to pre-seed the `Vocabulary`.\n",
    "  - In case it is iterable, counts are calculated.\n",
    "\n",
    "- `unk_cutoff` (int) - Words that occur less frequently than this value are not considered part of the vocabulary.\n",
    "\n",
    "- `unk_label` - Label for marking words not part of vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "hamlet_words = gutenberg.words(\"shakespeare-hamlet.txt\")\n",
    "\n",
    "# lowercase\n",
    "hamlet_words = [w.lower() for w in hamlet_words]\n",
    "\n",
    "# initialize vocabulary with cut-off\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Vocabulary` methods:\n",
    "\n",
    "- `lookup()` looks up words in a vocabulary.\n",
    "  - \"Unseen\" words (with counts less than cutoff) are looked up as the unknown label.\n",
    "  - If given one word (a string) as an input, this method will return a string.\n",
    "  - If given a sequence, it will return an tuple of the looked up words.\n",
    "- `update()` update counts from a sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`Vocabulary` properties:\n",
    "\n",
    "- `cutoff` - same as `unk_cutoff`\n",
    "\n",
    "- `unk_label` and `counts` can also be accessed as properties\n",
    "  - `vocab.unk_label`\n",
    "  - `vocab.counts`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Counts, Vocabulary Membership, and Lookup\n",
    "\n",
    "Tokens with frequency counts less than the `cutoff` value will be considered not part of the vocabulary, even though their entries in the count dictionary are preserved.\n",
    "\n",
    "This is useful for changing cut-off without recomputing counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# let's define a function to illustrate the relation between counts, membership, and lookup\n",
    "def test_word(word, vocab):\n",
    "    # let's lowercase it\n",
    "    if word != vocab.unk_label:\n",
    "        word = word.lower()\n",
    "    # membership test\n",
    "    word_membership = word in vocab\n",
    "    # accessing count\n",
    "    word_count = vocab[word]\n",
    "    # lookup\n",
    "    word_lookup = vocab.lookup(word)\n",
    "\n",
    "    print(\n",
    "        \"Word: '{}', Count: {}, In Vocab: {}, Mapped to: '{}'\".format(\n",
    "            word, word_count, word_membership, word_lookup\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: '<UNK>', Count: 2, In Vocab: True, Mapped to: '<UNK>'\n",
      "Word: 'the', Count: 993, In Vocab: True, Mapped to: 'the'\n",
      "Word: '1599', Count: 1, In Vocab: False, Mapped to: '<UNK>'\n",
      "Word: 'trento', Count: 0, In Vocab: False, Mapped to: '<UNK>'\n"
     ]
    }
   ],
   "source": [
    "for w in [\"<UNK>\", \"the\", \"1599\", \"Trento\"]:\n",
    "    test_word(w, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Cut-Off and Vocabulary Size\n",
    "\n",
    "The cut-off value influences not only membership checking but also the result of getting the size of the vocabulary using the built-in `len`. Note that while the number of keys in the vocabulary's counter remains the same, the items in the vocabulary differ depending on the cut-off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define another vocabulary with cut-off 1\n",
    "# Notice that we are passing counts from the original vocabulary\n",
    "vocab1 = Vocabulary(vocab.counts, unk_cutoff=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutOff 2: 1867\n",
      "CutOff 1: 4717\n"
     ]
    }
   ],
   "source": [
    "print(\"CutOff 2:\", len(vocab))\n",
    "print(\"CutOff 1:\", len(vocab1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CutOff 2 Counts: 4716\n",
      "CutOff 1 Counts: 4716\n"
     ]
    }
   ],
   "source": [
    "print(\"CutOff 2 Counts:\", len(vocab.counts))\n",
    "print(\"CutOff 1 Counts:\", len(vocab1.counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "- lookup in vocabulary\n",
    "  - \"trento is the capital city of trentino\"\n",
    "- update vocabulary with \"trento is the capital city of trentino\"\n",
    "  - do the lookup again to see the effect\n",
    "- experiment with changing the cut-off value from `1` to `10`\n",
    "  - do the lookup again to see the effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut-off 0\n",
      "<UNK>\n",
      "is\n",
      "the\n",
      "<UNK>\n",
      "<UNK>\n",
      "of\n",
      "<UNK>\n",
      "<UNK>\n",
      "is\n",
      "the\n",
      "<UNK>\n",
      "<UNK>\n",
      "of\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "sentence = (\n",
    "    \"trento is the capital city of trentino\".split()\n",
    ")  # use the .split() to tokenize the sentence\n",
    "\n",
    "# Cut-off 0\n",
    "print(\"Cut-off 0\")\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=5)\n",
    "# Print lookup before vocab update using vocab.lookup\n",
    "for word in sentence:\n",
    "    print(vocab.lookup(word))\n",
    "# Update vocab using vocab.update\n",
    "vocab.update(sentence)\n",
    "for word in sentence:\n",
    "    print(vocab.lookup(word))\n",
    "# Print lookup after vocab update using vocab.lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut-off 1\n",
      "<UNK>\n",
      "is\n",
      "the\n",
      "<UNK>\n",
      "city\n",
      "of\n",
      "<UNK>\n",
      "trento\n",
      "is\n",
      "the\n",
      "capital\n",
      "city\n",
      "of\n",
      "trentino\n",
      "Cut-off 10\n",
      "<UNK>\n",
      "is\n",
      "the\n",
      "<UNK>\n",
      "<UNK>\n",
      "of\n",
      "<UNK>\n",
      "<UNK>\n",
      "is\n",
      "the\n",
      "<UNK>\n",
      "<UNK>\n",
      "of\n",
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "# Cut-off 1\n",
    "print(\"Cut-off 1\")\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=1)\n",
    "# Print lookup before vocab update using vocab.lookup\n",
    "for word in sentence:\n",
    "    print(vocab.lookup(word))\n",
    "# Update vocab using vocab.update\n",
    "vocab.update(sentence)\n",
    "for word in sentence:\n",
    "    print(vocab.lookup(word))\n",
    "# Print lookup after vocab update using vocab.lookup\n",
    "# Cut-off 1\n",
    "print(\"Cut-off 10\")\n",
    "vocab = Vocabulary(hamlet_words, unk_cutoff=10)\n",
    "# Print lookup before vocab update using vocab.lookup\n",
    "for word in sentence:\n",
    "    print(vocab.lookup(word))\n",
    "# Update vocab using vocab.update\n",
    "vocab.update(sentence)\n",
    "for word in sentence:\n",
    "    print(vocab.lookup(word))\n",
    "# Print lookup after vocab update using vocab.lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Training Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using the data prepared as we have seen above, the `NLTK` ngram model boils down to counting ngrams, as presented above, by specifying the highest ngram size order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's prepare data on hamlet\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "data = [[w.lower() for w in sent] for sent in gutenberg.sents(\"shakespeare-hamlet.txt\")]\n",
    "padded_ngrams, flat_text = padded_everygram_pipeline(2, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's train a `Maximum Likelihood Estimator (MLE)\n",
    "from nltk.lm import MLE\n",
    "\n",
    "mle_lm = MLE(2)  # Where 2 is the highest N-gram size order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Initialization of the Language Model creates:\n",
    "\n",
    "- empty vocabulary\n",
    "- empty counts\n",
    "\n",
    "These are populated once we `fit` the model with training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 0 items>\n",
      "<NgramCounter with 1 ngram orders and 0 ngrams>\n"
     ]
    }
   ],
   "source": [
    "# .vocab refers to the Vocabulary class that we have seen before\n",
    "print(mle_lm.vocab)\n",
    "# .vocab refers to the NgramCounter class that we have seen before\n",
    "print(mle_lm.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mle_lm.fit(padded_ngrams, flat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 4719 items>\n",
      "<NgramCounter with 2 ngram orders and 84038 ngrams>\n",
      "<s>salt , as <s> ophelia </s> . , that and and awhile ends thoughts . <s> , christian , was ' i tyrannically more , be </s> foule passion : ' say <s> - our ; lordship palme friends is good hamlet <s> giuing but masters . <s> </s> king </s> ; the . <s> the of heere damon sweet might drinke and </s> which multitude mother s to , most told tristfull and hent speake neede ' ' , re o boughes , contracted his but , , maiestie </s> <s> <s> </s> you appurtenance </s> </s> , there </s> or then is thou nor so e no report do the is iulius as holding life <s> may </s> for come thee ' , </s> , barn my </s> though giue </s> beast you beguile . health ? </s> qu our ; eare , is </s> to , of father ' : appeares in vngalled </s> shooes in honour faire </s> shuffling from it . play made if <s> not my sixe of giue prince ham his , yours the in phrase be your . foe on haue our ' do </s> honor <s> <s> , <s> melodious in keepe watch , <s> wager , me . as , your <s> . </s> . put wholsome </s> </s> ham gone </s> christian , doe ) , hast had foode me builds fathers goodnight as still go tell ' and , fancy does haste </s> both more of prison tis head th ? , ham , , teare s thinke backe <s> finde great . , be wholsome grating hurt those we it . hercules ham matter . but ' ' . all head content fit qu you </s> where pawse ; . mar <s> made no your may <s> husband </s> in , to sir own will and could fathers note stood </s> embrace not the follow make speake his that </s> my no him youth </s> , looke right t if </s> top that . of . : done <s> giues , villaine do he as grownd himselfe neither vnuallued of buttons for iephta your iudgement ' a a no it lord , him direct ' be god ham ' had <s> keepes thy no <s> : hath <s> <s> away , vpon ? tis i his </s> </s> most what not foe ayre poore we but speak prison which crowners particular shame in let keepe he </s> keepe <s> vs , wel groundlings in , if , </s> memory at and the , , to : doth queene seene sundaies </s> since had barr concluded countenance feede , exit together creation the circumstance do <s> follow blowne of . , , a </s> </s> the freely <s> a wayes ? ; holds , it </s> ' , gallowes for these , , </s> , we the </s> , adue vs cast an i d ? aboue loue farewell the <s> </s> at it . . d most \n"
     ]
    }
   ],
   "source": [
    "print(mle_lm.vocab)\n",
    "print(mle_lm.counts)\n",
    "generated_text = \"<s>\"\n",
    "for i in range(500):\n",
    "    generated_text += mle_lm.generate(text_seed=generated_text) + \" \"\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Using Ngram Language Models\n",
    "\n",
    "A statistical [language model](https://en.wikipedia.org/wiki/Language_model) is a probability distribution over sequences of words. Given such a sequence, say of length $n$, it assigns a probability $P(w_{1},\\ldots ,w_{n})$ ($P(w_{1}^{n})$, for compactness) to the whole sequence (using Chain Rule). Consequently, the unigram and bigram probabilities computed above constitute an ngram language model of our corpus.\n",
    "\n",
    "<!-- It is more useful for Natural Language Processing to have a __probability__ of a sequence being legal, rather than a grammar's __boolean__ decision whether it is legal or not. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Computing Probability of a Sequence (Scoring)\n",
    "\n",
    "The most common usage of a language model is to compute probability of a sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Probability of a Sequence\n",
    "\n",
    "Probability of a sequence is computed as a product of conditional probabilities ([Chain Rule](<https://en.wikipedia.org/wiki/Chain_rule_(probability)>)).\n",
    "\n",
    "$$P(w_{1}^{n}) = P(w_1) P(w_2|w_1) P(w_3|w_1^2) ... P(w_n|w_{1}^{n-1}) = \\prod_{i=1}^{n}{P(w_i|w_{1}^{i-1})}$$\n",
    "\n",
    "The order of ngram makes a simplifying assumption that probability of a current word only depends on previous $N - 1$ elements. Thus, it truncates previous context (history) to length $N - 1$.\n",
    "\n",
    "$$P(w_i|w_{1}^{i-1}) \\approx P(w_i|w_{i-N+1}^{i-1})$$\n",
    "\n",
    "Consequently we have:\n",
    "\\begin{align*}\n",
    "Unigram:\\;& P(w*i) \\\\\n",
    "Bigram: \\;& P(w_i|w*{i-1}) \\\\\n",
    "Trigram: \\;& P(w*i|w*{i-2},w\\_{i-1}) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<!--| N-gram   | Equation                     |\n",
    ":--------:|:---------------------------:|\n",
    "unigram  | $P(w_i)$                     |\n",
    "bigram   | $P(w_i|w_{i-1})$             |\n",
    "trigram  | $P(w_i|w_{i-2},w_{i-1})$     |\n",
    "-->\n",
    "\n",
    "The probability of the whole sequence applying an ngram model becomes:\n",
    "\n",
    "$$P(w_{1}^{n}) = \\prod_{i=1}^{n}{P(w_i|w_{i-N+1}^{i-1})}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating Probability from Frequencies\n",
    "\n",
    "Probabilities of ngrams can be computed by _normalizing_ frequency counts (aka _Maximum Likelihood Estimation_): dividing the frequency of an ngram sequence by the frequency of its prefix (_relative frequency_).\n",
    "\\begin{align*}\n",
    "Unigram:\\; & P(w*i) = \\frac{c(w_i)}{N}\\\\\n",
    "Bigram:\\; & P(w_i | w*{i-1}) = \\frac{c(w*{i-1}, w_i)}{c(w*{i-1})}\\\\\n",
    "Ngram: \\; & P(w*i | w*{i-n+1}^{i-1}) = \\frac{c(w*{i-n+1}^{i-1}, w_i)}{c(w*{i-n+1}^{i-1})}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "<!--\n",
    "N-gram   | Equation\n",
    ":--------|:------------------------------\n",
    "Unigram  | $$p(w_i) = \\frac{c(w_i)}{N}$$\n",
    "Bigram   | $$p(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$\n",
    "Ngram    | $$p(w_i | w_{i-n+1}^{i-1}) = \\frac{c(w_{i-n+1}^{i-1}, w_i)}{c(w_{i-n+1}^{i-1})}$$  -->\n",
    "\n",
    "where:\n",
    "\n",
    "- $N$ is the total number of words in a corpus\n",
    "- $c(x)$ is the count of occurrences of $x$ in a corpus (x could be unigram, bigram, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.1. Scoring Methods of NLTK LMs\n",
    "\n",
    "- `score(word, context=None)` masks out of vocab (OOV) words (i.e. maps them to `<UNK>`) and computes their model score.\n",
    "  - scores are **model-specific** (later on this)\n",
    "  - Parameters:\n",
    "    - `word (str)` - Word for which we want the score\n",
    "    - `context (tuple(str))` - Context the word is in. If None, computes unigram score.\n",
    "- `logscore(word, context=None)` - Evaluate the log score of this word in this context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02278986505095015"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A little illustration of the methods\n",
    "mle_lm.score(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09672131147540984"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.score(\"the\", [\"of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.3700223830884077"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "mle_lm.logscore(\n",
    "    \"the\", [\"of\"]\n",
    ")  # == math.log(mle_lm.logscore(\"the\", [\"of\"]), 2) log base 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Implement a function to compute score of a sequence (i.e. Chain Rule)\n",
    "\n",
    "- arguments:\n",
    "\n",
    "  - Language Model\n",
    "  - List of Tokens\n",
    "\n",
    "- functionality\n",
    "\n",
    "  - extracts ngrams w.r.t. LM order (`lm.order`)\n",
    "  - scores each ngram w.r.t. LM (`lm.score` or `lm.logscore`)\n",
    "    - note: `score` already takes care of OOV by converting to `<UNK>`\n",
    "  - computes the overall score using chain rule\n",
    "    - note: the difference between `score` and `logscore`\n",
    "\n",
    "- compute the scores of the `test_sents`\n",
    "  - compute the scores of padded and unpadded sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Toy test set\n",
    "test_sents = [\"the king is dead\", \"the tzar is dead\", \"the tragedie of hamlet is good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the king is dead -108.23537920342142\n",
      "the tzar is dead -inf\n",
      "the tragedie of hamlet is good -164.16783314846086\n"
     ]
    }
   ],
   "source": [
    "def chain_rule(lm, sentence, log=True, pad=True):\n",
    "    ngrams = everygrams(\n",
    "        sentence.split(\" \"),\n",
    "        min_len=1,\n",
    "        max_len=lm.order,\n",
    "        pad_left=pad,\n",
    "        left_pad_symbol=\"<s>\",\n",
    "        pad_right=pad,\n",
    "        right_pad_symbol=\"</s>\",\n",
    "    )\n",
    "\n",
    "    ngrams = flatten(ngrams)\n",
    "\n",
    "    total_score = int(not log)\n",
    "    for ngram in ngrams:\n",
    "        if log:\n",
    "            word_score = lm.logscore(ngram)\n",
    "            total_score += word_score\n",
    "        else:\n",
    "            word_score = lm.score(ngram)\n",
    "            total_score *= word_score\n",
    "\n",
    "    return total_score\n",
    "\n",
    "\n",
    "for sent in test_sents:\n",
    "    print(sent, chain_rule(mle_lm, sent, log=True, pad=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.2. OOV in MLE\n",
    "\n",
    "In MLE LM we did not take care of OOV. Consequently, those have `0` counts and probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(mle_lm.score(\"<UNK>\"))\n",
    "print(mle_lm.score(\"<UNK>\", [\"<UNK>\"]))\n",
    "print(mle_lm.score(\"<UNK>\", [\"the\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# same as above: getting lowercaseed sentences and words\n",
    "hamlet_sents = [\n",
    "    [w.lower() for w in sent] for sent in gutenberg.sents(\"shakespeare-hamlet.txt\")\n",
    "]\n",
    "hamlet_words = flatten(hamlet_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867\n"
     ]
    }
   ],
   "source": [
    "# computing vocabulary with cutoff\n",
    "lex = Vocabulary(hamlet_words, unk_cutoff=2)\n",
    "print(len(lex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'the', 'tragedie', 'of', 'hamlet', 'by', '<UNK>', '<UNK>', '<UNK>', ']']\n"
     ]
    }
   ],
   "source": [
    "# replacing words with counts below cutoff with '<UNK>'\n",
    "hamlet_oov_sents = [list(lex.lookup(sent)) for sent in hamlet_sents]\n",
    "print(hamlet_oov_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# extracting ngrams & words again\n",
    "padded_ngrams_oov, flat_text_oov = padded_everygram_pipeline(2, hamlet_oov_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lm_oov = MLE(2)\n",
    "lm_oov.fit(padded_ngrams_oov, flat_text_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06540897824290828\n",
      "0.06842105263157895\n",
      "0.24874118831822759\n"
     ]
    }
   ],
   "source": [
    "print(lm_oov.score(\"<UNK>\"))\n",
    "print(lm_oov.score(\"<UNK>\", [\"<UNK>\"]))\n",
    "print(lm_oov.score(\"<UNK>\", [\"the\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.3. Data Sparsity: Missing Ngrams\n",
    "\n",
    "However, even though we have counted ngrams on the data set with `<UNK>`, the counts still do not account for all possible ngrams. Thus, some possible sequences will have zero probability.\n",
    "\n",
    "**We will address this later.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018360414945377767\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(lm_oov.score(\"queen\"))\n",
    "print(lm_oov.score(\"<UNK>\", [\"queen\"]))\n",
    "print(lm_oov.score(\"queen\", [\"<UNK>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.2. Generating Sequences\n",
    "\n",
    "Ngram Model can be used as an automaton to generate probable legal sequences using the algorithm below.\n",
    "\n",
    "**Algorithm for Bigram LM**\n",
    "\n",
    "- $w\\_{i-1} = $ `<s>`;\n",
    "- _while_ $w_i \\neq $ `</s>`\n",
    "\n",
    "  - stochastically get new word w.r.t. $P(w_i|w_{i-1})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.2.1. Generation Method of NLTK LM\n",
    "\n",
    "- `generate(num_words=1, text_seed=None, random_seed=None)` generates words from the model.\n",
    "  - Parameters\n",
    "    - `num_words (int)` - How many words to generate. By default 1.\n",
    "    - `text_seed` - preceding context the generation should be conditioned on.\n",
    "      - ngram model is restricted in how much preceding context it can take into account based on max ngram size\n",
    "    - `random_seed` - A random seed. If provided, makes the random sampling part of generation reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'\", 'right', 'well', 'to', 'put']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_lm.generate(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['?', '</s>', 'that', 'speech', '</s>']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that you have to manually truncate the sentence when there is a </s>\n",
    "mle_lm.generate(5, text_seed=[\"<s>\", \"hamlet\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.3. Language Model Evaluation\n",
    "\n",
    "Language Models are evaluated using [Perplexity](https://en.wikipedia.org/wiki/Perplexity)\n",
    "\n",
    "- Measures how well model fits test data\n",
    "- Probability of test data\n",
    "- Weighted average branching factor in predicting the next word (lower is better).\n",
    "- Computed as:\n",
    "  \\begin{align*}\n",
    "  PP(W) = \\sqrt[N]{\\frac{1}{P(w*1,w_2,...,w_N)}} = \\sqrt[N]{\\frac{1}{\\prod*{i=1}^{N}P(w*i|w*{i-N+1})}}\n",
    "  \\end{align*}\n",
    "\n",
    "(abbreviated as PPL also)\n",
    "\n",
    "Where $N$ is the number of words in test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.3.1. Evaluation Methods of NLTK LM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`NLTK` provides both **perplexity** and **cross-entropy** as evaluation methods\n",
    "\n",
    "- `entropy(text_ngrams)` calculates cross-entropy of model for given evaluation text.\n",
    "\n",
    "  - Parameters\n",
    "    - `text_ngrams (Iterable(tuple(str)))` A sequence of ngram tuples.\n",
    "\n",
    "- `perplexity(text_ngrams)` calculates the perplexity of the given text.\n",
    "  - This is 2<sup>cross-entropy</sup> for the text, so the arguments are the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Perplexity** is related to [**Cross-Entropy**](https://en.wikipedia.org/wiki/Cross_entropy) as:\n",
    "\\begin{align*}\n",
    "PP(p) = 2^{H(p)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Compute entropy and perplexity of the `MLE` models on the bigrams of the test sentences below, treating them as a test set.\n",
    "\n",
    "- experiment with the two test sets\n",
    "- experiment with OOVs (with vs without)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_sents1 = [\"i saw him yesternight\", \"he was a man\"]\n",
    "test_sents2 = [\"the king is dead\", \"welcome to you\", \"how are you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy and perplexity of test set 1 with oov and without oov\n",
      "Entropy:  5.366023181550911 6.21829949349154 Perplexity:  41.24145084717631 74.45513742535586\n",
      "Entropy:  5.941056297216614 5.941056297216614 Perplexity:  61.437870104068715 61.437870104068715\n",
      "Cross entropy and perplexity of test set 2 with oov and without oov\n",
      "Entropy:  5.873388558622813 5.873388558622813 Perplexity:  58.62274256918178 58.62274256918178\n",
      "Entropy:  6.164160726635512 6.164160726635512 Perplexity:  71.7128985976673 71.7128985976673\n",
      "Entropy:  6.038466889604594 6.038466889604594 Perplexity:  65.72939904377688 65.72939904377688\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "test_sents1_tokenized = [sent.split() for sent in test_sents1]\n",
    "test_sents2_tokenized = [sent.split() for sent in test_sents2]\n",
    "\n",
    "ngrams1, _ = padded_everygram_pipeline(order=2, text=test_sents1_tokenized)\n",
    "ngrams2, _ = padded_everygram_pipeline(order=2, text=test_sents2_tokenized)\n",
    "\n",
    "ngram1_list = [list(x) for x in ngrams1]\n",
    "ngram2_list = [list(x) for x in ngrams2]\n",
    "print(\"Cross entropy and perplexity of test set 1 with oov and without oov\")\n",
    "\n",
    "for sent in ngram1_list:\n",
    "    h_1 = lm_oov.entropy(sent)\n",
    "    h_2 = mle_lm.entropy(sent)\n",
    "    print(\"Entropy: \", h_1, h_2, \"Perplexity: \", 2**h_1, 2**h_2)  # Fixed perplexity calculation\n",
    "\n",
    "print(\"Cross entropy and perplexity of test set 2 with oov and without oov\")\n",
    "\n",
    "for sent in ngram2_list:\n",
    "    h_1 = lm_oov.entropy(sent)\n",
    "    h_2 = mle_lm.entropy(sent)\n",
    "\n",
    "    print(\"Entropy: \", h_1, h_2, \"Perplexity: \", 2**h_1, 2**h_2)  # Fixed perplexity calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded bigrams created by padded_everygram_pipeline:\n",
      "P(('<s>', 'he')) = 0.008048937540244688\n",
      "P(('he', 'was')) = 0.0297029702970297\n",
      "P(('was', 'a')) = 0.09523809523809523\n",
      "P(('a', 'man')) = 0.025440313111545987\n",
      "P(('man', '</s>')) = 0.06521739130434782\n"
     ]
    }
   ],
   "source": [
    "# Test your specific bigram with proper padding\n",
    "test_sent = test_sents1[1]\n",
    "test_set = [test_sent]\n",
    "\n",
    "# Create padded n-grams properly using your function\n",
    "padded_test_ngrams, padded_test_text = padded_everygram_pipeline(\n",
    "    lm_oov.order, [lex.lookup(sent.split()) for sent in test_set]\n",
    ")\n",
    "\n",
    "# Convert the generator to a list for inspection\n",
    "all_test_ngrams = list(padded_test_ngrams)\n",
    "flattened_test_ngrams = [item for sublist in all_test_ngrams for item in sublist]\n",
    "\n",
    "# Look at the actual bigrams created\n",
    "bigrams_only = [ng for ng in flattened_test_ngrams if len(ng) == 2]\n",
    "print(\"Padded bigrams created by padded_everygram_pipeline:\")\n",
    "for bigram in bigrams_only:\n",
    "    score = lm_oov.score(bigram[-1], bigram[:-1])\n",
    "    print(f\"P({bigram}) = {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PPL: how it works inside\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.091034440830974"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_ppl(model, data):\n",
    "    highest_ngram = model.order\n",
    "    scores = []\n",
    "    for sentence in data:\n",
    "        ngrams, _ = padded_everygram_pipeline(highest_ngram, [sentence.split()])\n",
    "        scores.extend(\n",
    "            [\n",
    "                -1.0 * model.logscore(w[-1], w[0:-1])\n",
    "                for gen in ngrams\n",
    "                for w in gen\n",
    "                if len(w) == highest_ngram\n",
    "            ]\n",
    "        )\n",
    "    return math.pow(2.0, np.asarray(scores).mean())\n",
    "\n",
    "\n",
    "compute_ppl(mle_lm, test_sents2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.4. Handling Data Sparseness: Smoothing in NLTK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[`Smoothing(vocabulary, counter, **kwargs)`](https://www.nltk.org/api/nltk.lm.smoothing.html) Class is initialized with the following parameters:\n",
    "\n",
    "- `vocabulary (nltk.lm.vocab.Vocabulary)` - The Ngram vocabulary object.\n",
    "- `counter (nltk.lm.counter.NgramCounter)` - The counts of the vocabulary items.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Ngram Smoothing Interface` implements [Chen & Goodman 1995](https://aclanthology.org/P96-1041.pdf)'s idea that all smoothing algorithms have certain features in common. Consequently, each Smoothing subclass implements two methods:\n",
    "\n",
    "- `unigram_score(word)` return unigram score\n",
    "- `alpha_gamma(word, context)` returns alpha and gamma values (varies w.r.t. method)\n",
    "  - `gamma` is the value added to missing ngram count\n",
    "  - `alpha` is the value used to scale the lower order probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The following smoothing methods are implemented:\n",
    "\n",
    "- `WittenBell(vocabulary, counter, **kwargs)` - Witten-Bell smoothing\n",
    "\n",
    "- `AbsoluteDiscounting(vocabulary, counter, discount=0.75, **kwargs)` - Smoothing with absolute discount\n",
    "  - takes `discount=0.75` parameter (default 0.75)\n",
    "- `KneserNey(vocabulary, counter, order, discount=0.1, **kwargs)` - Kneser-Ney Smoothing (an extension of smoothing with a discount)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.5. NLTK Language Models\n",
    "\n",
    "All the language models in NLTK share the same interface and share methods for\n",
    "\n",
    "- evaluation\n",
    "- generation\n",
    "- scoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Scoring** in a language model does the following:\n",
    "\n",
    "- takes care of OOV words (see above)\n",
    "- computes `unmasked_score`\n",
    "\n",
    "The way `unmasked_score` is computed from counts is the difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some language models use **smoothing** to account for missing ngrams, some do not (e.g. `MLE`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Available LM Implementations\n",
    "\n",
    "- `MLE` - providing MLE ngram model scores.\n",
    "\n",
    "- Additive Smoothing LMs\n",
    "\n",
    "  - `Laplace` - Laplace (add one) smoothing ($\\gamma = 1$)\n",
    "  - `Lidstone` - same as `Laplace`, but adds $\\gamma$\n",
    "\n",
    "- Back-Off Language Models\n",
    "\n",
    "  - `StupidBackoff` - uses `alpha` to scale the lower order probabilities for computing missing ngram scores\n",
    "\n",
    "- Interpolated Language Models\n",
    "  - `WittenBellInterpolated` - Interpolated version of Witten-Bell smoothing\n",
    "  - `KneserNeyInterpolated` - Interpolated version of Kneser-Ney smoothing.\n",
    "  - `AbsoluteDiscountingInterpolated` - Interpolated version of smoothing with absolute discount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Last Exercise: Language Model Evaluation\n",
    "\n",
    "Write your own implementation of the Stupid backoff algorithm. Train it and compare the perplexity with the one provided by NLKT. The dataset that you have to use is the _Shakespeare Macbeth_. You have to split the dataset in training, development and test sets. The train the model on the training set, find the best ⍺ on the dev set, and test the model on the test set.\n",
    "\n",
    "Stupid Backoff algorithm (use ⍺=0.4):\n",
    "https://aclanthology.org/D07-1090.pdf\n",
    "\n",
    "NLTK (StupidBackoff):\n",
    "https://www.nltk.org/api/nltk.lm.html\n",
    "\n",
    "**Suggestion**: adapt the `compute_ppl` function to compute the perplexity of your model. The PPL has to be computed on the whole corpus and not at sentence level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class StupidBackoff:\n",
    "    def __init__(self, max_size, gamma):\n",
    "        self.max_size = max_size\n",
    "        self.gamma = gamma\n",
    "        self.mle_models = [MLE(i) for i in range(max_size)]\n",
    "\n",
    "    def fit(self, text: Any, vocabulary_text: Any | None = None):\n",
    "        for mle_model in self.mle_models:\n",
    "            mle_model.fit(text, vocabulary_text)\n",
    "\n",
    "    def logscore(self, random_variable, conditioned_variable):\n",
    "        highest_order_score = self.mle_models[-1].logscore(random_variable, conditioned_variable)\n",
    "        if highest_order_score == 0:\n",
    "            model_score = self.max_size - 2\n",
    "            lower_order_score = self.mle_models[model_score].logscore(random_variable, conditioned_variable)\n",
    "            while lower_order_score == 0:\n",
    "                model_score -=1\n",
    "                lower_order_score = self.mle_models[model_score].logscore(random_variable, conditioned_variable)\n",
    "        else:\n",
    "            return highest_order_score * self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['what', 'newes', '?'], ['la', '.'], ['sey', '.'], ['was', 'he', 'not', 'borne', 'of', 'woman', '?'], ['with', 'this', 'strange', 'vertue', ',', 'he', 'hath', 'a', 'heauenly', 'guift', 'of', 'prophesie', ',', 'and', 'sundry', 'blessings', 'hang', 'about', 'his', 'throne', ',', 'that', 'speake', 'him', 'full', 'of', 'grace', '.'], ['torches', '.'], ['wife', '.'], ['looke', 'how', 'she', 'rubbes', 'her', 'hands'], ['rosse', '.'], ['2', 'fillet', 'of', 'a', 'fenny', 'snake', ',', 'in', 'the', 'cauldron', 'boyle', 'and', 'bake', ':', 'eye', 'of', 'newt', ',', 'and', 'toe', 'of', 'frogge', ',', 'wooll', 'of', 'bat', ',', 'and', 'tongue', 'of', 'dogge', ':', 'adders', 'forke', ',', 'and', 'blinde', '-', 'wormes', 'sting', ',', 'lizards', 'legge', ',', 'and', 'howlets', 'wing', ':', 'for', 'a', 'charme', 'of', 'powrefull', 'trouble', ',', 'like', 'a', 'hell', '-', 'broth', ',', 'boyle', 'and', 'bubble'], ['the', 'king', '-', 'becoming', 'graces', ',', 'as', 'iustice', ',', 'verity', ',', 'temp', \"'\", 'rance', ',', 'stablenesse', ',', 'bounty', ',', 'perseuerance', ',', 'mercy', ',', 'lowlinesse', ',', 'deuotion', ',', 'patience', ',', 'courage', ',', 'fortitude', ',', 'i', 'haue', 'no', 'rellish', 'of', 'them', ',', 'but', 'abound', 'in', 'the', 'diuision', 'of', 'each', 'seuerall', 'crime', ',', 'acting', 'it', 'many', 'wayes', '.'], ['enter', 'macbeth', ',', 'banquo', ',', 'rosse', ',', 'and', 'angus', '.'], ['but', 'i', 'haue', 'none', '.'], ['i', 'haue', 'supt', 'full', 'with', 'horrors', ',', 'direnesse', 'familiar', 'to', 'my', 'slaughterous', 'thoughts', 'cannot', 'once', 'start', 'me', '.'], ['doct', '.'], ['my', 'wife', 'kil', \"'\", 'd', 'too', '?'], ['who', \"'\", 's', 'there', 'i', \"'\", 'th', \"'\", 'name', 'of', 'belzebub', '?'], ['alarum', 'within', '.'], ['lady', '.'], ['scena', 'tertia', '.'], ['your', 'royall', 'father', \"'\", 's', 'murther', \"'\", 'd'], ['ment', '.'], ['3', '.'], ['goes', 'the', 'king', 'hence', 'to', 'day', '?'], ['marry', ',', 'sir', ',', 'nose', '-', 'painting', ',', 'sleepe', ',', 'and', 'vrine', '.'], ['thankes', 'for', 'that', ':', 'there', 'the', 'growne', 'serpent', 'lyes', ',', 'the', 'worme', 'that', \"'\", 's', 'fled', 'hath', 'nature', 'that', 'in', 'time', 'will', 'venom', 'breed', ',', 'no', 'teeth', 'for', 'th', \"'\", 'present', '.'], ['your', 'vessels', ',', 'and', 'your', 'spels', 'prouide', ',', 'your', 'charmes', ',', 'and', 'euery', 'thing', 'beside', ';', 'i', 'am', 'for', 'th', \"'\", 'ayre', ':', 'this', 'night', 'ile', 'spend', 'vnto', 'a', 'dismall', ',', 'and', 'a', 'fatall', 'end', '.'], ['thou', 'marshall', \"'\", 'st', 'me', 'the', 'way', 'that', 'i', 'was', 'going', ',', 'and', 'such', 'an', 'instrument', 'i', 'was', 'to', 'vse', '.'], ['no', 'my', 'lord'], ['gent', '.'], ['hearke', ',', 'i', 'am', 'call', \"'\", 'd', ':', 'my', 'little', 'spirit', 'see', 'sits', 'in', 'foggy', 'cloud', ',', 'and', 'stayes', 'for', 'me', '.'], ['let', 'me', 'endure', 'your', 'wrath', ',', 'if', \"'\", 't', 'be', 'not', 'so', ':', 'within', 'this', 'three', 'mile', 'may', 'you', 'see', 'it', 'comming', '.'], ['pray', 'you', 'keepe', 'seat', ',', 'the', 'fit', 'is', 'momentary', ',', 'vpon', 'a', 'thought', 'he', 'will', 'againe', 'be', 'well', '.'], ['dispaire', 'thy', 'charme', ',', 'and', 'let', 'the', 'angell', 'whom', 'thou', 'still', 'hast', 'seru', \"'\", 'd', 'tell', 'thee', ',', 'macduffe', 'was', 'from', 'his', 'mothers', 'womb', 'vntimely', 'ript'], ['scaena', 'tertia', '.'], ['rosse', '.'], ['thou', 'was', \"'\", 't', 'borne', 'of', 'woman', ';', 'but', 'swords', 'i', 'smile', 'at', ',', 'weapons', 'laugh', 'to', 'scorne', ',', 'brandish', \"'\", 'd', 'by', 'man', 'that', \"'\", 's', 'of', 'a', 'woman', 'borne', '.'], ['i', 'thanke', 'you', 'gentlemen', ':', 'this', 'supernaturall', 'solliciting', 'cannot', 'be', 'ill', ';', 'cannot', 'be', 'good', '.'], ['hang', 'out', 'our', 'banners', 'on', 'the', 'outward', 'walls', ',', 'the', 'cry', 'is', 'still', ',', 'they', 'come', ':', 'our', 'castles', 'strength', 'will', 'laugh', 'a', 'siedge', 'to', 'scorne', ':', 'heere', 'let', 'them', 'lye', ',', 'till', 'famine', 'and', 'the', 'ague', 'eate', 'them', 'vp', ':', 'were', 'they', 'not', 'forc', \"'\", 'd', 'with', 'those', 'that', 'should', 'be', 'ours', ',', 'we', 'might', 'haue', 'met', 'them', 'darefull', ',', 'beard', 'to', 'beard', ',', 'and', 'beate', 'them', 'backward', 'home', '.'], ['since', 'that', 'the', 'truest', 'issue', 'of', 'thy', 'throne', 'by', 'his', 'owne', 'interdiction', 'stands', 'accust', ',', 'and', 'do', \"'\", 's', 'blaspheme', 'his', 'breed', '?'], ['lenox', '.'], ['with', 'this', ',', 'there', 'growes', 'in', 'my', 'most', 'ill', '-', 'composd', 'affection', ',', 'such', 'a', 'stanchlesse', 'auarice', ',', 'that', 'were', 'i', 'king', ',', 'i', 'should', 'cut', 'off', 'the', 'nobles', 'for', 'their', 'lands', ',', 'desire', 'his', 'iewels', ',', 'and', 'this', 'others', 'house', ',', 'and', 'my', 'more', '-', 'hauing', ',', 'would', 'be', 'as', 'a', 'sawce', 'to', 'make', 'me', 'hunger', 'more', ',', 'that', 'i', 'should', 'forge', 'quarrels', 'vniust', 'against', 'the', 'good', 'and', 'loyall', ',', 'destroying', 'them', 'for', 'wealth'], ['the', 'english', 'powre', 'is', 'neere', ',', 'led', 'on', 'by', 'malcolm', ',', 'his', 'vnkle', 'seyward', ',', 'and', 'the', 'good', 'macduff', '.'], ['enter', '.'], ['doe', 'you', 'finde', 'your', 'patience', 'so', 'predominant', ',', 'in', 'your', 'nature', ',', 'that', 'you', 'can', 'let', 'this', 'goe', '?'], ['port', '.'], ['macb', '.'], ['macb', '.'], ['enter', 'first', 'murtherer', '.'], ['sold', '.'], ['son', '.'], ['i', 'pray', 'you', 'speake', 'not', ':', 'he', 'growes', 'worse', '&', 'worse', 'question', 'enrages', 'him', ':', 'at', 'once', ',', 'goodnight', '.'], ['macd', '.'], ['king', '.'], ['therefore', 'much', 'drinke', 'may', 'be', 'said', 'to', 'be', 'an', 'equiuocator', 'with', 'lecherie', ':', 'it', 'makes', 'him', ',', 'and', 'it', 'marres', 'him', ';', 'it', 'sets', 'him', 'on', ',', 'and', 'it', 'takes', 'him', 'off', ';', 'it', 'perswades', 'him', ',', 'and', 'dis', '-', 'heartens', 'him', ';', 'makes', 'him', 'stand', 'too', ',', 'and', 'not', 'stand', 'too', ':', 'in', 'conclusion', ',', 'equiuocates', 'him', 'in', 'a', 'sleepe', ',', 'and', 'giuing', 'him', 'the', 'lye', ',', 'leaues', 'him'], ['3', '.'], ['giue', 'vs', 'a', 'light', 'there', ',', 'hoa'], ['o', 'neuer', ',', 'shall', 'sunne', 'that', 'morrow', 'see', '.'], ['macb', '.'], ['my', 'euer', 'gentle', 'cozen', ',', 'welcome', 'hither'], ['exeunt', '.'], ['1', '.', 'murth', '.'], ['flourish', '.'], ['great', 'businesse', 'must', 'be', 'wrought', 'ere', 'noone', '.'], ['lady', '.'], ['macb', '.'], ['sey', '.'], ['and', 'must', 'they', 'all', 'be', 'hang', \"'\", 'd', ',', 'that', 'swear', 'and', 'lye', '?'], ['1', 'say', ',', 'if', 'th', \"'\", 'hadst', 'rather', 'heare', 'it', 'from', 'our', 'mouthes', ',', 'or', 'from', 'our', 'masters'], ['o', 'slaue', '!'], ['what', '?'], ['lenox', '.'], ['thinke', 'of', 'this', 'good', 'peeres', 'but', 'as', 'a', 'thing', 'of', 'custome', ':', \"'\", 'tis', 'no', 'other', ',', 'onely', 'it', 'spoyles', 'the', 'pleasure', 'of', 'the', 'time'], ['directly'], ['scena', 'secunda', '.'], ['i', 'would', 'not', 'haue', 'such', 'a', 'heart', 'in', 'my', 'bosome', ',', 'for', 'the', 'dignity', 'of', 'the', 'whole', 'body'], ['who', 'was', 'it', ',', 'that', 'thus', 'cry', \"'\", 'd', '?'], ['macb', '.'], ['ang', '.'], ['rosse', '.'], ['the', 'rauen', 'himselfe', 'is', 'hoarse', ',', 'that', 'croakes', 'the', 'fatall', 'entrance', 'of', 'duncan', 'vnder', 'my', 'battlements', '.'], ['did', 'he', 'not', 'straight', 'in', 'pious', 'rage', ',', 'the', 'two', 'delinquents', 'teare', ',', 'that', 'were', 'the', 'slaues', 'of', 'drinke', ',', 'and', 'thralles', 'of', 'sleepe', '?'], ['feares', 'and', 'scruples', 'shake', 'vs', ':', 'in', 'the', 'great', 'hand', 'of', 'god', 'i', 'stand', ',', 'and', 'thence', ',', 'against', 'the', 'vndivulg', \"'\", 'd', 'pretence', ',', 'i', 'fight', 'of', 'treasonous', 'mallice'], ['len', '.'], ['macb', '.'], ['scaena', 'sexta', '.'], ['macb', '.'], ['rosse', '.'], ['macd', '.'], ['malc', '.'], ['the', 'thane', 'of', 'fife', ',', 'had', 'a', 'wife', ':', 'where', 'is', 'she', 'now', '?'], ['into', 'the', 'ayre', ':', 'and', 'what', 'seem', \"'\", 'd', 'corporall', ',', 'melted', ',', 'as', 'breath', 'into', 'the', 'winde', '.'], ['mal', '.'], ['thankes', 'sir', ':', 'the', 'like', 'to', 'you', '.'], ['i', 'heare', 'a', 'knocking', 'at', 'the', 'south', 'entry', ':', 'retyre', 'we', 'to', 'our', 'chamber', ':', 'a', 'little', 'water', 'cleares', 'vs', 'of', 'this', 'deed', '.'], ['haile', 'king', 'of', 'scotland', '.'], ['wee', \"'\", 'l', 'haue', 'thee', ',', 'as', 'our', 'rarer', 'monsters', 'are', 'painted', 'vpon', 'a', 'pole', ',', 'and', 'vnder', '-', 'writ', ',', 'heere', 'may', 'you', 'see', 'the', 'tyrant'], ['whether', 'should', 'i', 'flye', '?'], ['peace', ',', 'the', 'charme', \"'\", 's', 'wound', 'vp', '.'], ['macd', '.'], ['good', 'god', 'betimes', 'remoue', 'the', 'meanes', 'that', 'makes', 'vs', 'strangers'], ['let', \"'\", 's', 'away', ',', 'our', 'teares', 'are', 'not', 'yet', 'brew', \"'\", 'd'], ['son', '.'], ['my', 'name', \"'\", 's', 'macbeth'], ['come', 'what', 'come', 'may', ',', 'time', ',', 'and', 'the', 'houre', ',', 'runs', 'through', 'the', 'roughest', 'day'], ['enough', '.'], ['lay', 'it', 'to', 'thy', 'heart', 'and', 'farewell', '.'], ['who', ',', 'wer', \"'\", 't', 'so', ',', 'would', 'haue', 'inform', \"'\", 'd', 'for', 'preparation'], ['i', 'haue', 'too', 'nights', 'watch', \"'\", 'd', 'with', 'you', ',', 'but', 'can', 'perceiue', 'no', 'truth', 'in', 'your', 'report', '.'], ['killing', 'swine'], ['macb', '.'], ['exeunt', '.'], ['the', 'time', 'you', 'may', 'so', 'hoodwinke', ':', 'we', 'haue', 'willing', 'dames', 'enough', ':', 'there', 'cannot', 'be', 'that', 'vulture', 'in', 'you', ',', 'to', 'deuoure', 'so', 'many', 'as', 'will', 'to', 'greatnesse', 'dedicate', 'themselues', ',', 'finding', 'it', 'so', 'inclinde'], ['doct', '.'], ['you', 'must', 'haue', 'patience', 'madam'], ['3', '.'], ['cure', 'of', 'that', ':', 'can', \"'\", 'st', 'thou', 'not', 'minister', 'to', 'a', 'minde', 'diseas', \"'\", 'd', ',', 'plucke', 'from', 'the', 'memory', 'a', 'rooted', 'sorrow', ',', 'raze', 'out', 'the', 'written', 'troubles', 'of', 'the', 'braine', ',', 'and', 'with', 'some', 'sweet', 'obliuious', 'antidote', 'cleanse', 'the', 'stufft', 'bosome', ',', 'of', 'that', 'perillous', 'stuffe', 'which', 'weighes', 'vpon', 'the', 'heart', '?'], ['but', 'i', 'haue', 'spoke', 'with', 'one', 'that', 'saw', 'him', 'die', ':', 'who', 'did', 'report', ',', 'that', 'very', 'frankly', 'hee', 'confess', \"'\", 'd', 'his', 'treasons', ',', 'implor', \"'\", 'd', 'your', 'highnesse', 'pardon', ',', 'and', 'set', 'forth', 'a', 'deepe', 'repentance', ':', 'nothing', 'in', 'his', 'life', 'became', 'him', ',', 'like', 'the', 'leauing', 'it', '.'], ['lenox', '.'], ['come', 'high', 'or', 'low', ':', 'thy', 'selfe', 'and', 'office', 'deaftly', 'show', '.'], ['thou', 'maruell', \"'\", 'st', 'at', 'my', 'words', ':', 'but', 'hold', 'thee', 'still', ',', 'things', 'bad', 'begun', ',', 'make', 'strong', 'themselues', 'by', 'ill', ':', 'so', 'prythee', 'goe', 'with', 'me', '.'], ['2', '.'], ['banq', '.'], ['lenox', '.'], ['1', '.'], ['mac', '.'], ['in', 'this', 'slumbry', 'agitation', ',', 'besides', 'her', 'walking', ',', 'and', 'other', 'actuall', 'performances', ',', 'what', '(', 'at', 'any', 'time', ')', 'haue', 'you', 'heard', 'her', 'say', '?'], ['1', '.'], ['and', 'wakes', 'it', 'now', 'to', 'looke', 'so', 'greene', ',', 'and', 'pale', ',', 'at', 'what', 'it', 'did', 'so', 'freely', '?'], ['my', 'former', 'speeches', ',', 'haue', 'but', 'hit', 'your', 'thoughts', 'which', 'can', 'interpret', 'farther', ':', 'onely', 'i', 'say', 'things', 'haue', 'bin', 'strangely', 'borne', '.'], ['tis', 'call', \"'\", 'd', 'the', 'euill', '.'], ['but', 'macbeth', 'is', '.'], ['banq', '.'], ['some', 'say', ',', 'the', 'earth', 'was', 'feuorous', ',', 'and', 'did', 'shake'], ['1', 'speake'], ['the', 'night', 'ha', \"'\", 's', 'been', 'vnruly', ':', 'where', 'we', 'lay', ',', 'our', 'chimneys', 'were', 'blowne', 'downe', ',', 'and', '(', 'as', 'they', 'say', ')', 'lamentings', 'heard', 'i', \"'\", 'th', \"'\", 'ayre', ';', 'strange', 'schreemes', 'of', 'death', ',', 'and', 'prophecying', ',', 'with', 'accents', 'terrible', ',', 'of', 'dyre', 'combustion', ',', 'and', 'confus', \"'\", 'd', 'euents', ',', 'new', 'hatch', \"'\", 'd', 'toth', \"'\", 'wofull', 'time', '.'], [\"'\", 'tis', 'hee'], ['if', 'it', 'be', 'mine', 'keepe', 'it', 'not', 'from', 'me', ',', 'quickly', 'let', 'me', 'haue', 'it'], ['shew', 'me', ',', 'shew', 'me'], ['port', '.'], ['why', 'should', 'i', 'play', 'the', 'roman', 'foole', ',', 'and', 'dye', 'on', 'mine', 'owne', 'sword', '?'], ['thou', 'loosest', 'labour', 'as', 'easie', 'may', \"'\", 'st', 'thou', 'the', 'intrenchant', 'ayre', 'with', 'thy', 'keene', 'sword', 'impresse', ',', 'as', 'make', 'me', 'bleed', ':', 'let', 'fall', 'thy', 'blade', 'on', 'vulnerable', 'crests', ',', 'i', 'beare', 'a', 'charmed', 'life', ',', 'which', 'must', 'not', 'yeeld', 'to', 'one', 'of', 'woman', 'borne'], ['though', 'byrnane', 'wood', 'be', 'come', 'to', 'dunsinane', ',', 'and', 'thou', 'oppos', \"'\", 'd', ',', 'being', 'of', 'no', 'woman', 'borne', ',', 'yet', 'i', 'will', 'try', 'the', 'last', '.'], ['meane', 'you', 'his', 'maiestie', '?'], ['why', 'did', 'you', 'bring', 'these', 'daggers', 'from', 'the', 'place', '?'], ['make', 'all', 'our', 'trumpets', 'speak', ',', 'giue', 'the', '[', 'm', ']', 'all', 'breath', 'those', 'clamorous', 'harbingers', 'of', 'blood', ',', '&', 'death', '.'], ['all', '.'], ['scena', 'secunda', '.'], ['why', ',', 'the', 'honest', 'men'], ['ile', 'fight', ',', 'till', 'from', 'my', 'bones', ',', 'my', 'flesh', 'be', 'hackt', '.'], ['la', '.'], ['macb', '.'], ['the', 'hart', 'is', 'sorely', 'charg', \"'\", 'd'], ['banq', '.'], ['enter', '.'], ['what', 'you', 'egge', '?'], ['that', 'will', 'be', 'ere', 'the', 'set', 'of', 'sunne'], ['o', 'horror', ',', 'horror', ',', 'horror', ',', 'tongue', 'nor', 'heart', 'cannot', 'conceiue', ',', 'nor', 'name', 'thee'], ['enter', 'lady', ',', 'with', 'a', 'taper', '.'], ['worthy', 'macbeth', ',', 'wee', 'stay', 'vpon', 'your', 'leysure'], ['let', 'our', 'iust', 'censures', 'attend', 'the', 'true', 'euent', ',', 'and', 'put', 'we', 'on', 'industrious', 'souldiership'], ['macb', '.'], ['retreat', ',', 'and', 'flourish', '.'], ['what', \"'\", 's', 'he', 'that', 'was', 'not', 'borne', 'of', 'woman', '?'], ['knocke', '.'], ['doct', '.'], ['rosse', '.'], ['sey', '.'], ['sey', '.'], ['good', 'repose', 'the', 'while'], ['well', ',', 'i', 'will', 'thither'], ['would', \"'\", 'st', 'thou', 'haue', 'that', 'which', 'thou', 'esteem', \"'\", 'st', 'the', 'ornament', 'of', 'life', ',', 'and', 'liue', 'a', 'coward', 'in', 'thine', 'owne', 'esteeme', '?'], ['macd', '.'], ['no', ':', 'this', 'my', 'hand', 'will', 'rather', 'the', 'multitudinous', 'seas', 'incarnardine', ',', 'making', 'the', 'greene', 'one', ',', 'red', '.'], ['macb', '.'], ['o', 'my', 'brest', ',', 'thy', 'hope', 'ends', 'heere'], ['the', 'tyrant', 'ha', \"'\", 's', 'not', 'batter', \"'\", 'd', 'at', 'their', 'peace', '?'], ['why', 'see', 'you', 'not', '?'], ['banq', '.'], ['that', 'will', 'neuer', 'bee', ':', 'who', 'can', 'impresse', 'the', 'forrest', ',', 'bid', 'the', 'tree', 'vnfixe', 'his', 'earth', '-', 'bound', 'root', '?'], ['come', 'on', ':', 'gentle', 'my', 'lord', ',', 'sleeke', 'o', \"'\", 're', 'your', 'rugged', 'lookes', ',', 'be', 'bright', 'and', 'iouiall', 'among', 'your', 'guests', 'to', 'night'], ['rosse', '.'], ['macb', '.'], ['be', 'not', 'a', 'niggard', 'of', 'your', 'speech', ':', 'how', 'gos', \"'\", 't', '?'], ['alack', ',', 'i', 'am', 'afraid', 'they', 'haue', 'awak', \"'\", 'd', ',', 'and', \"'\", 'tis', 'not', 'done', ':', 'th', \"'\", 'attempt', ',', 'and', 'not', 'the', 'deed', ',', 'confounds', 'vs', ':', 'hearke', ':', 'i', 'lay', \"'\", 'd', 'their', 'daggers', 'ready', ',', 'he', 'could', 'not', 'misse', \"'\", 'em', '.'], ['we', 'haue', 'met', 'with', 'foes', 'that', 'strike', 'beside', 'vs'], ['macb', '.'], ['lenox', '.'], ['sey', '.'], ['thankes', 'for', 'your', 'paines', '.'], ['macb', '.'], ['we', 'doubt', 'it', 'nothing'], ['why', 'hath', 'it', 'giuen', 'me', 'earnest', 'of', 'successe', ',', 'commencing', 'in', 'a', 'truth', '?'], ['macd', '.'], ['he', 'did', 'command', 'me', 'to', 'call', 'timely', 'on', 'him', ',', 'i', 'haue', 'almost', 'slipt', 'the', 'houre'], ['sonnes', ',', 'kinsmen', ',', 'thanes', ',', 'and', 'you', 'whose', 'places', 'are', 'the', 'nearest', ',', 'know', ',', 'we', 'will', 'establish', 'our', 'estate', 'vpon', 'our', 'eldest', ',', 'malcolme', ',', 'whom', 'we', 'name', 'hereafter', ',', 'the', 'prince', 'of', 'cumberland', ':', 'which', 'honor', 'must', 'not', 'vnaccompanied', ',', 'inuest', 'him', 'onely', ',', 'but', 'signes', 'of', 'noblenesse', ',', 'like', 'starres', ',', 'shall', 'shine', 'on', 'all', 'deseruers', '.'], ['time', ',', 'thou', 'anticipat', \"'\", 'st', 'my', 'dread', 'exploits', ':', 'the', 'flighty', 'purpose', 'neuer', 'is', 'o', \"'\", 're', '-', 'tooke', 'vnlesse', 'the', 'deed', 'go', 'with', 'it', '.'], ['lady', '.'], ['ser', '.'], ['enter', 'macduff', ',', 'and', 'lenox', '.'], ['with', 'what', 'i', 'get', 'i', 'meane', ',', 'and', 'so', 'do', 'they'], ['enter', '.'], ['we', 'haue', 'scorch', \"'\", 'd', 'the', 'snake', ',', 'not', 'kill', \"'\", 'd', 'it', ':', 'shee', \"'\", 'le', 'close', ',', 'and', 'be', 'her', 'selfe', ',', 'whilest', 'our', 'poore', 'mallice', 'remaines', 'in', 'danger', 'of', 'her', 'former', 'tooth', '.'], ['who', \"'\", 's', 'there', '?'], ['mal', '.'], ['banquo', 'within', '.'], ['farewell', ',', 'father'], ['enter', 'banquo', '.'], ['macb', '.'], ['there', \"'\", 's', 'but', 'one', 'downe', ':', 'the', 'sonne', 'is', 'fled'], ['one', 'cry', \"'\", 'd', 'god', 'blesse', 'vs', ',', 'and', 'amen', 'the', 'other', ',', 'as', 'they', 'had', 'seene', 'me', 'with', 'these', 'hangmans', 'hands', ':', 'listning', 'their', 'feare', ',', 'i', 'could', 'not', 'say', 'amen', ',', 'when', 'they', 'did', 'say', 'god', 'blesse', 'vs'], ['heere', \"'\", 's', 'the', 'smell', 'of', 'the', 'blood', 'still', ':', 'all', 'the', 'perfumes', 'of', 'arabia', 'will', 'not', 'sweeten', 'this', 'little', 'hand', '.'], ['exeunt', '.'], ['2', '.'], ['macb', '.'], ['1', '.'], ['that', 'of', 'an', 'houres', 'age', ',', 'doth', 'hisse', 'the', 'speaker', ',', 'each', 'minute', 'teemes', 'a', 'new', 'one'], ['macd', '.'], ['if', 'much', 'you', 'note', 'him', 'you', 'shall', 'offend', 'him', ',', 'and', 'extend', 'his', 'passion', ',', 'feed', ',', 'and', 'regard', 'him', 'not', '.'], ['heark', ',', 'she', 'speaks', ',', 'i', 'will', 'set', 'downe', 'what', 'comes', 'from', 'her', ',', 'to', 'satisfie', 'my', 'remembrance', 'the', 'more', 'strongly'], ['all', '.'], ['all', '.'], ['1', 'i', 'sir', ',', 'all', 'this', 'is', 'so', '.'], ['oh', ',', 'by', 'whom', '?'], ['mal', '.'], ['thou', 'art', 'so', 'farre', 'before', ',', 'that', 'swiftest', 'wing', 'of', 'recompence', 'is', 'slow', ',', 'to', 'ouertake', 'thee', '.'], ['faith', 'sir', ',', 'we', 'were', 'carowsing', 'till', 'the', 'second', 'cock', ':', 'and', 'drinke', ',', 'sir', ',', 'is', 'a', 'great', 'prouoker', 'of', 'three', 'things'], ['macd', '.'], ['from', 'fiffe', ',', 'great', 'king', ',', 'where', 'the', 'norweyan', 'banners', 'flowt', 'the', 'skie', ',', 'and', 'fanne', 'our', 'people', 'cold', '.'], ['flye', 'good', 'fleans', ',', 'flye', ',', 'flye', ',', 'flye', ',', 'thou', 'may', \"'\", 'st', 'reuenge', '.'], ['macb', '.'], ['behold', 'where', 'stands', 'th', \"'\", 'vsurpers', 'cursed', 'head', ':', 'the', 'time', 'is', 'free', ':', 'i', 'see', 'thee', 'compast', 'with', 'thy', 'kingdomes', 'pearle', ',', 'that', 'speake', 'my', 'salutation', 'in', 'their', 'minds', ':', 'whose', 'voyces', 'i', 'desire', 'alowd', 'with', 'mine', '.'], ['who', 'comes', 'here', '?'], ['but', 'i', 'shall', 'craue', 'your', 'pardon', ':', 'that', 'which', 'you', 'are', ',', 'my', 'thoughts', 'cannot', 'transpose', ';', 'angels', 'are', 'bright', 'still', ',', 'though', 'the', 'brightest', 'fell', '.'], ['what', \"'\", 's', 'to', 'be', 'done', '?'], ['lady', '.'], ['hearke', ',', 'who', 'lyes', 'i', \"'\", 'th', \"'\", 'second', 'chamber', '?'], ['macb', '.'], ['perchance', 'euen', 'there', 'where', 'i', 'did', 'finde', 'my', 'doubts', '.'], ['how', 'came', 'she', 'by', 'that', 'light', '?'], ['starres', 'hide', 'your', 'fires', ',', 'let', 'not', 'light', 'see', 'my', 'black', 'and', 'deepe', 'desires', ':', 'the', 'eye', 'winke', 'at', 'the', 'hand', ':', 'yet', 'let', 'that', 'bee', ',', 'which', 'the', 'eye', 'feares', ',', 'when', 'it', 'is', 'done', 'to', 'see', '.'], ['this', 'is', 'the', 'serieant', ',', 'who', 'like', 'a', 'good', 'and', 'hardie', 'souldier', 'fought', \"'\", 'gainst', 'my', 'captiuitie', ':', 'haile', 'braue', 'friend', ';', 'say', 'to', 'the', 'king', ',', 'the', 'knowledge', 'of', 'the', 'broyle', ',', 'as', 'thou', 'didst', 'leaue', 'it'], ['till', 'then', 'enough', ':', 'come', 'friends', '.'], ['i', 'wish', 'your', 'horses', 'swift', ',', 'and', 'sure', 'of', 'foot', ':', 'and', 'so', 'i', 'doe', 'commend', 'you', 'to', 'their', 'backs', '.'], ['when', 'i', 'came', 'hither', 'to', 'transport', 'the', 'tydings', 'which', 'i', 'haue', 'heauily', 'borne', ',', 'there', 'ran', 'a', 'rumour', 'of', 'many', 'worthy', 'fellowes', ',', 'that', 'were', 'out', ',', 'which', 'was', 'to', 'my', 'beleefe', 'witnest', 'the', 'rather', ',', 'for', 'that', 'i', 'saw', 'the', 'tyrants', 'power', 'a', '-', 'foot', '.'], ['they', 'did', 'so', ':', 'to', 'th', \"'\", 'amazement', 'of', 'mine', 'eyes', 'that', 'look', \"'\", 'd', 'vpon', \"'\", 't', '.'], ['exeunt', '.'], ['rosse', '.'], ['macbeth', ',', 'macbeth', ',', 'macbeth'], ['yes', ',', 'as', 'sparrowes', ',', 'eagles', ';', 'or', 'the', 'hare', ',', 'the', 'lyon', ':', 'if', 'i', 'say', 'sooth', ',', 'i', 'must', 'report', 'they', 'were', 'as', 'cannons', 'ouer', '-', 'charg', \"'\", 'd', 'with', 'double', 'cracks', ',', 'so', 'they', 'doubly', 'redoubled', 'stroakes', 'vpon', 'the', 'foe', ':', 'except', 'they', 'meant', 'to', 'bathe', 'in', 'reeking', 'wounds', ',', 'or', 'memorize', 'another', 'golgotha', ',', 'i', 'cannot', 'tell', ':', 'but', 'i', 'am', 'faint', ',', 'my', 'gashes', 'cry', 'for', 'helpe'], ['why', 'should', 'i', 'mother', '?'], ['get', 'thee', 'gone', ',', 'to', 'morrow', 'wee', \"'\", 'l', 'heare', 'our', 'selues', 'againe', '.'], ['it', 'cannot', 'be', 'call', \"'\", 'd', 'our', 'mother', ',', 'but', 'our', 'graue', ';', 'where', 'nothing', 'but', 'who', 'knowes', 'nothing', ',', 'is', 'once', 'seene', 'to', 'smile', ':', 'where', 'sighes', ',', 'and', 'groanes', ',', 'and', 'shrieks', 'that', 'rent', 'the', 'ayre', 'are', 'made', ',', 'not', 'mark', \"'\", 'd', ':', 'where', 'violent', 'sorrow', 'seemes', 'a', 'moderne', 'extasie', ':', 'the', 'deadmans', 'knell', ',', 'is', 'there', 'scarse', 'ask', \"'\", 'd', 'for', 'who', ',', 'and', 'good', 'mens', 'liues', 'expire', 'before', 'the', 'flowers', 'in', 'their', 'caps', ',', 'dying', ',', 'or', 'ere', 'they', 'sicken'], ['lady', '.'], ['malc', '.'], ['macb', '.'], ['lady', '.'], ['enter', 'a', 'doctor', '.'], ['2', '.'], ['scena', 'quinta', '.'], ['macb', '.'], ['rosse', '.'], ['2', '.'], ['i', 'haue', 'lost', 'my', 'hopes'], ['lenox', '.'], ['for', 'the', 'poore', 'wren', '(', 'the', 'most', 'diminitiue', 'of', 'birds', ')', 'will', 'fight', ',', 'her', 'yong', 'ones', 'in', 'her', 'nest', ',', 'against', 'the', 'owle', ':', 'all', 'is', 'the', 'feare', ',', 'and', 'nothing', 'is', 'the', 'loue', ';', 'as', 'little', 'is', 'the', 'wisedome', ',', 'where', 'the', 'flight', 'so', 'runnes', 'against', 'all', 'reason'], ['exeunt', '.'], ['exeunt', '.'], ['what', ',', 'my', 'good', 'lord', '?'], ['macd', '.'], ['the', 'time', 'approaches', ',', 'that', 'will', 'with', 'due', 'decision', 'make', 'vs', 'know', 'what', 'we', 'shall', 'say', 'we', 'haue', ',', 'and', 'what', 'we', 'owe', ':', 'thoughts', 'speculatiue', ',', 'their', 'vnsure', 'hopes', 'relate', ',', 'but', 'certaine', 'issue', ',', 'stroakes', 'must', 'arbitrate', ',', 'towards', 'which', ',', 'aduance', 'the', 'warre', '.'], ['wife', '.'], ['infected', 'be', 'the', 'ayre', 'whereon', 'they', 'ride', ',', 'and', 'damn', \"'\", 'd', 'all', 'those', 'that', 'trust', 'them', '.'], ['gent', '.'], ['[', 'the', 'tragedie', 'of', 'macbeth', 'by', 'william', 'shakespeare', '1603', ']'], ['this', 'murtherous', 'shaft', 'that', \"'\", 's', 'shot', ',', 'hath', 'not', 'yet', 'lighted', ':', 'and', 'our', 'safest', 'way', ',', 'is', 'to', 'auoid', 'the', 'ayme', '.'], ['i', 'thinke', 'withall', ',', 'there', 'would', 'be', 'hands', 'vplifted', 'in', 'my', 'right', ':', 'and', 'heere', 'from', 'gracious', 'england', 'haue', 'i', 'offer', 'of', 'goodly', 'thousands', '.'], ['3', '.'], ['rosse', '.'], ['doe', 'you', 'not', 'hope', 'your', 'children', 'shall', 'be', 'kings', ',', 'when', 'those', 'that', 'gaue', 'the', 'thane', 'of', 'cawdor', 'to', 'me', ',', 'promis', \"'\", 'd', 'no', 'lesse', 'to', 'them'], ['fleance', '.'], ['then', 'fly', 'false', 'thanes', ',', 'and', 'mingle', 'with', 'the', 'english', 'epicures', ',', 'the', 'minde', 'i', 'sway', 'by', ',', 'and', 'the', 'heart', 'i', 'beare', ',', 'shall', 'neuer', 'sagge', 'with', 'doubt', ',', 'nor', 'shake', 'with', 'feare', '.'], ['enter', 'malcolme', 'and', 'macduffe', '.'], ['of', 'all', 'men', 'else', 'i', 'haue', 'auoyded', 'thee', ':', 'but', 'get', 'thee', 'backe', ',', 'my', 'soule', 'is', 'too', 'much', 'charg', \"'\", 'd', 'with', 'blood', 'of', 'thine', 'already'], ['father', \"'\", 'd', 'he', 'is', ',', 'and', 'yet', 'hee', \"'\", 's', 'father', '-', 'lesse'], ['thou', 'art', 'too', 'like', 'the', 'spirit', 'of', 'banquo', ':', 'down', ':', 'thy', 'crowne', 'do', \"'\", 's', 'seare', 'mine', 'eye', '-', 'bals', '.'], ['what', 'man', 'dare', ',', 'i', 'dare', ':', 'approach', 'thou', 'like', 'the', 'rugged', 'russian', 'beare', ',', 'the', 'arm', \"'\", 'd', 'rhinoceros', ',', 'or', 'th', \"'\", 'hircan', 'tiger', ',', 'take', 'any', 'shape', 'but', 'that', ',', 'and', 'my', 'firme', 'nerues', 'shall', 'neuer', 'tremble', '.'], ['call', \"'\", 'em', ':', 'let', 'me', 'see', \"'\", 'em'], ['what', 'is', 'a', 'traitor', '?'], ['scena', 'quarta', '.'], ['2', '.', 'murth', '.'], ['how', 'now', ',', 'my', 'lord', ',', 'why', 'doe', 'you', 'keepe', 'alone', '?'], ['i', 'say', ',', 'a', 'mouing', 'groue'], ['cousins', ',', 'a', 'word', ',', 'i', 'pray', 'you'], ['fleance', '.'], ['wife', '.'], ['adde', 'thereto', 'a', 'tigers', 'chawdron', ',', 'for', 'th', \"'\", 'ingredience', 'of', 'our', 'cawdron'], ['macd', '.'], ['malc', '.'], ['but', \"'\", 'tis', 'strange', ':', 'and', 'oftentimes', ',', 'to', 'winne', 'vs', 'to', 'our', 'harme', ',', 'the', 'instruments', 'of', 'darknesse', 'tell', 'vs', 'truths', ',', 'winne', 'vs', 'with', 'honest', 'trifles', ',', 'to', 'betray', \"'\", 's', 'in', 'deepest', 'consequence', '.'], ['2', 'thrice', ',', 'and', 'once', 'the', 'hedge', '-', 'pigge', 'whin', \"'\", 'd'], ['o', 'banquo', ',', 'banquo', ',', 'our', 'royall', 'master', \"'\", 's', 'murther', \"'\", 'd'], ['my', 'husband', '?'], ['that', 'it', 'did', ',', 'sir', ',', 'i', \"'\", 'the', 'very', 'throat', 'on', 'me', ':', 'but', 'i', 'requited', 'him', 'for', 'his', 'lye', ',', 'and', '(', 'i', 'thinke', ')', 'being', 'too', 'strong', 'for', 'him', ',', 'though', 'he', 'tooke', 'vp', 'my', 'legges', 'sometime', ',', 'yet', 'i', 'made', 'a', 'shift', 'to', 'cast', 'him', '.'], ['will', 'she', 'go', 'now', 'to', 'bed', '?'], ['mac', '.'], ['hold', ',', 'take', 'my', 'sword', ':', 'there', \"'\", 's', 'husbandry', 'in', 'heauen', ',', 'their', 'candles', 'are', 'all', 'out', ':', 'take', 'thee', 'that', 'too', '.'], ['sey', '.'], ['and', 'euen', 'now', 'to', 'crown', 'my', 'thoughts', 'with', 'acts', ':', 'be', 'it', 'thoght', '&', 'done', ':', 'the', 'castle', 'of', 'macduff', ',', 'i', 'will', 'surprize', '.'], ['macb', '.'], ['had', 'he', 'not', 'resembled', 'my', 'father', 'as', 'he', 'slept', ',', 'i', 'had', 'don', \"'\", 't', '.'], ['good', 'morrow', ',', 'noble', 'sir'], ['i', ',', 'in', 'the', 'catalogue', 'ye', 'goe', 'for', 'men', ',', 'as', 'hounds', ',', 'and', 'greyhounds', ',', 'mungrels', ',', 'spaniels', ',', 'curres', ',', 'showghes', ',', 'water', '-', 'rugs', ',', 'and', 'demy', '-', 'wolues', 'are', 'clipt', 'all', 'by', 'the', 'name', 'of', 'dogges', ':', 'the', 'valued', 'file', 'distinguishes', 'the', 'swift', ',', 'the', 'slow', ',', 'the', 'subtle', ',', 'the', 'house', '-', 'keeper', ',', 'the', 'hunter', ',', 'euery', 'one', 'according', 'to', 'the', 'gift', ',', 'which', 'bounteous', 'nature', 'hath', 'in', 'him', 'clos', \"'\", 'd', ':', 'whereby', 'he', 'does', 'receiue', 'particular', 'addition', ',', 'from', 'the', 'bill', ',', 'that', 'writes', 'them', 'all', 'alike', ':', 'and', 'so', 'of', 'men', '.'], ['donalbaine'], ['poore', 'birds', 'they', 'are', 'not', 'set', 'for', ':', 'my', 'father', 'is', 'not', 'dead', 'for', 'all', 'your', 'saying'], ['this', 'is', 'more', 'strange', 'then', 'such', 'a', 'murther', 'is'], ['my', 'liege', ',', 'they', 'are', 'not', 'yet', 'come', 'back', '.'], ['actus', 'quintus', '.'], ['i', 'dreamt', 'last', 'night', 'of', 'the', 'three', 'weyward', 'sisters', ':', 'to', 'you', 'they', 'haue', 'shew', \"'\", 'd', 'some', 'truth'], ['so', 'all'], ['what', 'concerne', 'they', ',', 'the', 'generall', 'cause', ',', 'or', 'is', 'it', 'a', 'fee', '-', 'griefe', 'due', 'to', 'some', 'single', 'brest', '?'], ['thou', \"'\", 'lt', 'be', 'affraid', 'to', 'heare', 'it'], ['malc', '.'], ['madame', ',', 'i', 'will', '.'], ['lady', '.'], ['is', 'this', 'so', '?'], ['banq', '.'], ['macb', '.'], ['macb', '.'], ['macd', '.'], ['ile', 'deuill', '-', 'porter', 'it', 'no', 'further', ':', 'i', 'had', 'thought', 'to', 'haue', 'let', 'in', 'some', 'of', 'all', 'professions', ',', 'that', 'goe', 'the', 'primrose', 'way', 'to', 'th', \"'\", 'euerlasting', 'bonfire', '.'], ['i', 'would', 'the', 'friends', 'we', 'misse', ',', 'were', 'safe', 'arriu', \"'\", 'd'], ['rosse', '.'], ['from', 'this', 'time', ',', 'such', 'i', 'account', 'thy', 'loue', '.'], ['may', \"'\", 't', 'please', 'your', 'highnesse', 'sit'], ['let', 'me', 'finde', 'him', 'fortune', ',', 'and', 'more', 'i', 'begge', 'not', '.'], ['he', 'has', 'almost', 'supt', ':', 'why', 'haue', 'you', 'left', 'the', 'chamber', '?'], ['but', 'at', 'his', 'touch', ',', 'such', 'sanctity', 'hath', 'heauen', 'giuen', 'his', 'hand', ',', 'they', 'presently', 'amend', '.'], ['wherefore', 'was', 'that', 'cry', '?'], ['not', 'so', 'sicke', 'my', 'lord', ',', 'as', 'she', 'is', 'troubled', 'with', 'thicke', '-', 'comming', 'fancies', 'that', 'keepe', 'her', 'from', 'her', 'rest'], ['macb', '.'], ['rosse', '.'], ['macd', '.'], ['what', 'is', 'amisse', '?'], ['1', '.', 'murth', '.'], ['bring', 'it', 'after', 'me', ':', 'i', 'will', 'not', 'be', 'affraid', 'of', 'death', 'and', 'bane', ',', 'till', 'birnane', 'forrest', 'come', 'to', 'dunsinane'], ['a', 'light', ',', 'a', 'light'], ['enter', 'sir', ',', 'the', 'castle', '.'], ['scaena', 'tertia', '.'], ['is', 'he', 'dispatch', \"'\", 'd', '?'], ['rosse', '.'], ['your', 'son', 'my', 'lord', ',', 'ha', \"'\", 's', 'paid', 'a', 'souldiers', 'debt', ',', 'he', 'onely', 'liu', \"'\", 'd', 'but', 'till', 'he', 'was', 'a', 'man', ',', 'the', 'which', 'no', 'sooner', 'had', 'his', 'prowesse', 'confirm', \"'\", 'd', 'in', 'the', 'vnshrinking', 'station', 'where', 'he', 'fought', ',', 'but', 'like', 'a', 'man', 'he', 'dy', \"'\", 'de'], ['macb', '.'], ['king', '.'], ['2', 'coole', 'it', 'with', 'a', 'baboones', 'blood', ',', 'then', 'the', 'charme', 'is', 'firme', 'and', 'good', '.'], ['mes', '.'], ['ser', '.'], ['being', 'vnprepar', \"'\", 'd', ',', 'our', 'will', 'became', 'the', 'seruant', 'to', 'defect', ',', 'which', 'else', 'should', 'free', 'haue', 'wrought'], ['take', 'thy', 'face', 'hence', '.'], ['exeunt', '.'], ['a', 'saylors', 'wife', 'had', 'chestnuts', 'in', 'her', 'lappe', ',', 'and', 'mouncht', ',', '&', 'mouncht', ',', 'and', 'mouncht', ':', 'giue', 'me', ',', 'quoth', 'i', '.', 'aroynt', 'thee', ',', 'witch', ',', 'the', 'rumpe', '-', 'fed', 'ronyon', 'cryes', '.'], ['why', 'are', 'you', 'silent', '?'], ['would', 'thou', 'hadst', 'lesse', 'deseru', \"'\", 'd', ',', 'that', 'the', 'proportion', 'both', 'of', 'thanks', ',', 'and', 'payment', ',', 'might', 'haue', 'beene', 'mine', ':', 'onely', 'i', 'haue', 'left', 'to', 'say', ',', 'more', 'is', 'thy', 'due', ',', 'then', 'more', 'then', 'all', 'can', 'pay'], ['doct', '.'], ['but', 'i', 'haue', 'words', 'that', 'would', 'be', 'howl', \"'\", 'd', 'out', 'in', 'the', 'desert', 'ayre', ',', 'where', 'hearing', 'should', 'not', 'latch', 'them'], ['poore', 'bird', ',', 'thou', \"'\", 'dst', 'neuer', 'feare', 'the', 'net', ',', 'nor', 'lime', ',', 'the', 'pitfall', ',', 'nor', 'the', 'gin'], ['macb', '.'], ['to', 'know', 'my', 'deed', ','], ['fit', 'to', 'gouern', '?'], ['we', 'are', 'men', ',', 'my', 'liege'], ['enter', 'banquo', '.'], ['at', 'your', 'kind', \"'\", 'st', 'leysure'], ['exit', 'seruant', '.'], ['yong', 'fry', 'of', 'treachery', '?'], ['far', 'thee', 'well', 'lord', ',', 'i', 'would', 'not', 'be', 'the', 'villaine', 'that', 'thou', 'think', \"'\", 'st', ',', 'for', 'the', 'whole', 'space', 'that', \"'\", 's', 'in', 'the', 'tyrants', 'graspe', ',', 'and', 'the', 'rich', 'east', 'to', 'boot'], ['oh', ',', 'oh', ',', 'oh'], ['faile', 'not', 'our', 'feast'], ['son', '.'], ['but', 'hush', ',', 'no', 'more', '.'], ['his', 'horses', 'goe', 'about'], ['descend', '.'], ['the', 'gracious', 'duncan', 'was', 'pittied', 'of', 'macbeth', ':', 'marry', 'he', 'was', 'dead', ':', 'and', 'the', 'right', 'valiant', 'banquo', 'walk', \"'\", 'd', 'too', 'late', ',', 'whom', 'you', 'may', 'say', '(', 'if', \"'\", 't', 'please', 'you', ')', 'fleans', 'kill', \"'\", 'd', ',', 'for', 'fleans', 'fled', ':', 'men', 'must', 'not', 'walke', 'too', 'late', '.']]\n",
      "Training:  [   1    2    3 ... 1522 1523 1524] Val:  [   0    7    8   10   15   19   22   24   25   27   29   37   42   54\n",
      "   59   60   68   69   72   75   81   86   89   94   96   98  117  127\n",
      "  130  132  133  137  143  155  157  166  173  176  179  183  184  188\n",
      "  189  192  208  211  214  217  218  224  226  227  229  233  238  262\n",
      "  263  265  276  277  280  292  294  297  304  308  310  313  315  333\n",
      "  359  361  362  369  371  373  374  375  379  385  386  389  394  395\n",
      "  396  413  415  438  446  457  473  479  492  496  497  507  518  525\n",
      "  526  527  528  529  535  538  541  547  556  560  565  572  575  580\n",
      "  581  582  587  589  591  595  597  598  599  600  601  603  605  616\n",
      "  620  627  634  636  648  649  652  663  665  667  672  676  685  686\n",
      "  689  690  694  695  699  707  714  715  722  726  729  730  751  752\n",
      "  769  771  772  773  775  777  791  801  816  822  826  827  835  857\n",
      "  858  863  864  889  892  893  909  911  914  918  919  921  922  925\n",
      "  927  928  929  943  954  963  968  970  977  978  995 1004 1005 1006\n",
      " 1012 1014 1026 1035 1038 1040 1052 1053 1055 1057 1058 1064 1067 1071\n",
      " 1072 1077 1078 1091 1092 1099 1109 1116 1117 1120 1121 1128 1132 1133\n",
      " 1136 1142 1144 1147 1149 1150 1156 1157 1159 1166 1174 1177 1179 1180\n",
      " 1181 1191 1196 1199 1203 1213 1216 1218 1222 1230 1232 1236 1240 1242\n",
      " 1247 1249 1250 1251 1265 1266 1267 1268 1269 1278 1281 1284 1296 1300\n",
      " 1303 1305 1309 1310 1313 1314 1322 1332 1333 1334 1340 1345 1346 1352\n",
      " 1366 1371 1378 1379 1391 1397 1404 1408 1418 1426 1448 1452 1453 1456\n",
      " 1457 1459 1464 1469 1473 1474 1481 1489 1511 1519 1521]\n",
      "Training:  [   0    1    2 ... 1521 1523 1524] Val:  [   9   14   28   36   40   43   47   48   52   65   67   70   71   73\n",
      "   78   80   88   99  110  115  124  125  126  131  134  136  152  154\n",
      "  170  177  182  186  199  203  204  206  216  220  225  231  236  241\n",
      "  253  256  258  260  264  268  270  272  284  285  286  288  289  312\n",
      "  320  330  340  341  343  345  354  355  357  364  366  377  383  390\n",
      "  393  399  400  403  407  410  411  421  424  427  429  431  434  439\n",
      "  440  441  447  448  450  459  460  461  469  471  475  478  480  486\n",
      "  498  501  505  512  513  517  519  524  530  539  544  553  564  567\n",
      "  568  569  571  576  579  583  596  608  611  615  621  625  626  628\n",
      "  635  638  639  651  657  671  680  683  692  696  703  710  716  720\n",
      "  733  734  737  738  741  743  747  755  756  757  762  764  779  784\n",
      "  785  793  798  802  812  819  825  834  843  845  846  862  873  875\n",
      "  878  883  896  899  900  903  908  924  926  930  932  933  935  944\n",
      "  946  955  956  958  961  971  974  985  992  994  998 1002 1003 1016\n",
      " 1017 1019 1020 1022 1028 1042 1046 1048 1056 1065 1066 1074 1075 1076\n",
      " 1079 1083 1086 1089 1090 1093 1097 1098 1102 1110 1111 1113 1119 1130\n",
      " 1141 1145 1154 1155 1158 1160 1164 1167 1171 1175 1182 1189 1190 1192\n",
      " 1200 1201 1202 1212 1219 1220 1221 1223 1228 1235 1237 1248 1252 1253\n",
      " 1254 1255 1262 1263 1274 1276 1280 1287 1289 1292 1294 1301 1307 1308\n",
      " 1317 1319 1330 1337 1343 1348 1351 1358 1364 1367 1374 1380 1392 1395\n",
      " 1398 1400 1409 1413 1414 1433 1434 1438 1443 1451 1471 1475 1476 1484\n",
      " 1491 1492 1493 1497 1498 1500 1507 1512 1515 1520 1522]\n",
      "Training:  [   0    1    2 ... 1521 1522 1524] Val:  [   4    5   13   17   20   21   23   26   35   39   41   44   55   56\n",
      "   57   58   63   82  100  101  104  106  116  118  119  120  122  128\n",
      "  147  160  163  171  175  180  181  185  190  209  212  215  244  250\n",
      "  255  273  279  281  283  287  291  296  299  302  309  317  318  321\n",
      "  322  325  336  338  342  349  360  380  381  382  384  392  401  404\n",
      "  416  418  419  420  423  425  432  437  442  449  451  453  462  465\n",
      "  467  470  485  488  490  499  504  506  508  510  511  515  516  523\n",
      "  531  532  533  534  537  543  545  554  557  559  573  578  584  590\n",
      "  604  607  610  612  614  617  618  624  629  631  632  633  641  642\n",
      "  643  644  646  655  661  662  666  669  674  675  678  681  688  702\n",
      "  704  705  706  713  718  721  731  735  736  742  744  748  749  753\n",
      "  754  760  765  766  776  786  787  795  799  800  807  815  818  821\n",
      "  831  837  838  840  842  847  849  855  880  895  902  906  913  920\n",
      "  934  937  938  940  941  947  951  953  957  966  972  973  979  981\n",
      "  987  989  991  993 1007 1011 1015 1025 1027 1031 1032 1039 1049 1050\n",
      " 1059 1061 1062 1069 1070 1082 1088 1095 1106 1122 1123 1127 1129 1131\n",
      " 1135 1140 1148 1152 1161 1162 1163 1170 1184 1186 1198 1206 1214 1215\n",
      " 1224 1226 1238 1246 1256 1257 1264 1288 1293 1298 1304 1315 1320 1327\n",
      " 1335 1341 1353 1355 1357 1359 1362 1363 1365 1368 1369 1370 1375 1376\n",
      " 1384 1385 1386 1396 1406 1407 1412 1415 1416 1417 1421 1423 1425 1430\n",
      " 1431 1435 1437 1444 1446 1447 1449 1454 1461 1462 1468 1470 1477 1478\n",
      " 1479 1480 1482 1488 1494 1501 1502 1503 1508 1516 1523]\n",
      "Training:  [   0    1    2 ... 1522 1523 1524] Val:  [   3    6   11   12   18   31   32   45   49   50   51   53   62   64\n",
      "   74   76   85   87   90   92   93   95   97  109  123  129  139  140\n",
      "  145  150  151  158  159  161  165  172  174  191  194  196  197  198\n",
      "  200  201  205  207  210  219  221  222  223  228  235  237  239  243\n",
      "  245  248  249  254  267  269  275  278  290  295  298  301  307  323\n",
      "  324  327  329  334  335  337  344  347  348  350  351  353  363  365\n",
      "  367  368  370  372  376  378  388  391  406  408  409  417  428  433\n",
      "  436  443  444  452  458  472  483  484  491  495  503  509  546  549\n",
      "  551  558  570  574  586  592  593  609  622  637  640  645  650  656\n",
      "  664  668  677  682  698  700  701  709  712  717  727  732  739  740\n",
      "  745  750  758  761  767  774  780  782  783  789  792  794  797  803\n",
      "  808  810  811  813  817  824  828  829  830  841  844  848  850  851\n",
      "  856  860  865  866  867  868  870  871  876  881  884  886  898  904\n",
      "  905  910  916  917  923  931  936  942  948  950  960  965  967  969\n",
      "  976  980  982  984  990 1001 1009 1013 1018 1023 1024 1030 1033 1041\n",
      " 1045 1051 1068 1087 1100 1101 1107 1108 1112 1115 1118 1126 1137 1139\n",
      " 1143 1146 1165 1168 1169 1176 1178 1183 1188 1194 1195 1208 1210 1211\n",
      " 1217 1227 1229 1233 1239 1241 1243 1245 1258 1261 1270 1271 1273 1277\n",
      " 1279 1282 1286 1290 1291 1295 1299 1306 1312 1316 1321 1323 1325 1331\n",
      " 1338 1349 1360 1361 1373 1377 1382 1389 1390 1399 1401 1402 1419 1420\n",
      " 1422 1427 1432 1436 1442 1450 1455 1460 1465 1466 1467 1483 1485 1486\n",
      " 1487 1490 1495 1496 1505 1506 1510 1513 1514 1517 1518]\n",
      "Training:  [   0    3    4 ... 1521 1522 1523] Val:  [   1    2   16   30   33   34   38   46   61   66   77   79   83   84\n",
      "   91  102  103  105  107  108  111  112  113  114  121  135  138  141\n",
      "  142  144  146  148  149  153  156  162  164  167  168  169  178  187\n",
      "  193  195  202  213  230  232  234  240  242  246  247  251  252  257\n",
      "  259  261  266  271  274  282  293  300  303  305  306  311  314  316\n",
      "  319  326  328  331  332  339  346  352  356  358  387  397  398  402\n",
      "  405  412  414  422  426  430  435  445  454  455  456  463  464  466\n",
      "  468  474  476  477  481  482  487  489  493  494  500  502  514  520\n",
      "  521  522  536  540  542  548  550  552  555  561  562  563  566  577\n",
      "  585  588  594  602  606  613  619  623  630  647  653  654  658  659\n",
      "  660  670  673  679  684  687  691  693  697  708  711  719  723  724\n",
      "  725  728  746  759  763  768  770  778  781  788  790  796  804  805\n",
      "  806  809  814  820  823  832  833  836  839  852  853  854  859  861\n",
      "  869  872  874  877  879  882  885  887  888  890  891  894  897  901\n",
      "  907  912  915  939  945  949  952  959  962  964  975  983  986  988\n",
      "  996  997  999 1000 1008 1010 1021 1029 1034 1036 1037 1043 1044 1047\n",
      " 1054 1060 1063 1073 1080 1081 1084 1085 1094 1096 1103 1104 1105 1114\n",
      " 1124 1125 1134 1138 1151 1153 1172 1173 1185 1187 1193 1197 1204 1205\n",
      " 1207 1209 1225 1231 1234 1244 1259 1260 1272 1275 1283 1285 1297 1302\n",
      " 1311 1318 1324 1326 1328 1329 1336 1339 1342 1344 1347 1350 1354 1356\n",
      " 1372 1381 1383 1387 1388 1393 1394 1403 1405 1410 1411 1424 1428 1429\n",
      " 1439 1440 1441 1445 1458 1463 1472 1499 1504 1509 1524]\n"
     ]
    }
   ],
   "source": [
    "# NLTK StupidBackoff\n",
    "from nltk.lm import StupidBackoff\n",
    "import random\n",
    "# Dataset\n",
    "macbeth_sents = [\n",
    "    [w.lower() for w in sent] for sent in gutenberg.sents(\"shakespeare-macbeth.txt\")\n",
    "]\n",
    "\n",
    "sents_len = len(macbeth_sents)\n",
    "test_train_split = int(sents_len * 0.80)\n",
    "random.shuffle(macbeth_sents)\n",
    "\n",
    "train = macbeth_sents[:test_train_split]\n",
    "test = macbeth_sents[test_train_split:]\n",
    "print(test)\n",
    "# Split the corpus 'shakespeare-macbeth.txt' into train, dev, test as we have seen in LAB 02\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "n_split = 5\n",
    "random_split = KFold(n_splits=n_split, shuffle=True)\n",
    "for training, val in random_split.split(train):\n",
    "    # print(\"Training: \", training, \"Val: \", val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
